{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AnkaFlow - Run Data Pipelines Anywhere","text":"<p>From REST APIs to SQL, from Local Python to Browser Execution</p>"},{"location":"#what-is-ankaflow","title":"What is AnkaFlow?","text":"<p>AnkaFlow is a YAML-driven, SQL-powered data pipeline framework designed for both local Python and in-browser (Pyodide) execution. It enables seamless extraction, transformation, and joining of data across REST APIs, cloud storage, and databases, all without writing custom Python code.</p> <p>Write your pipeline once, run it anywhere.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Dual Execution Modes: Run pipelines locally or fully in-browser with Pyodide.</li> <li>DuckDB In-Memory SQL Engine: Fast, scalable analytics with SQL.</li> <li>Dynamic Templating: Full support for variable injection, header and query templating.</li> <li>REST &amp; GraphQL Support: Production-ready REST and GraphQL connectors with error handling and polling.</li> <li>Joins Across REST and SQL: Native support for combining API responses with SQL datasets.</li> <li>Python Transform Stage: Execute custom Python logic inline within your pipeline.</li> <li>DeltaLake, BigQuery, S3, MSSQL, Oracle: Seamlessly connect to enterprise data sources.</li> <li>YAML Anchors &amp; References: DRY pipeline definitions with reusable components.</li> <li>Async Ready and Future-Proof: Designed for scalable and parallel execution.</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Data Enrichment Pipelines: Join Shopify orders (REST), DeltaLake financials, BigQuery users, and real-time weather data.</li> <li>Browser-Based Data Apps: Execute pipelines directly in the browser, preserving data privacy.</li> <li>ML Feature Engineering: Combine SQL and Python transform steps for complex feature generation.</li> <li>SaaS Product Integrations: Embed pipelines into dashboards, trigger REST calls, and process responses.</li> <li>Ad-hoc Analysis and Reporting: Dynamic pipelines for analysts and consultants, no Python code required.</li> </ul>"},{"location":"#why-choose-ankaflow","title":"Why Choose AnkaFlow?","text":""},{"location":"#ankaflow-vs-other-pipeline-frameworks","title":"AnkaFlow vs Other Pipeline Frameworks","text":"Feature AnkaFlow Airflow Dagster Bonobo Luigi DLT In-Browser Execution (Pyodide) \u2705 Yes \u274c \u274c \u274c \u274c \u274c Dynamic Templating \u2705 Yes \ud83d\udd36 Partial (Jinja) \ud83d\udd36 Partial \ud83d\udd36 Basic \ud83d\udd36 Basic \ud83d\udd36 via Python REST + SQL Join \u2705 Native \ud83d\udd36 Plugin-based \ud83d\udd36 Possible \ud83d\udd36 Indirect \ud83d\udd36 Indirect \ud83d\udd36 via SQLMesh Python Transform \u2705 Yes \ud83d\udd36 Plugin-based \u2705 Yes \u2705 Yes \u2705 Yes \u2705 Yes Pure SQL Transforms \u2705 Native (DuckDB SQL) \ud83d\udd36 via Plugins \ud83d\udd36 Limited SQL Nodes \u274c \u274c \u2705 via Destinations **BigQuery / Delta / S3 ** \u2705 Native Support \ud83d\udd36 via Plugins \u2705 Integrations \ud83d\udd36 User-managed \ud83d\udd36 User-managed \u2705 Native Recursive YAML / Anchors \u2705 Yes \ud83d\udd36 via Jinja \ud83d\udd36 Partial \u274c \u274c \u274c External System Requirements \u2705 None \u2014 self-contained \u274c Requires DB &amp; Scheduler \ud83d\udd36 Optional Metadata DB \u2705 Lightweight \u2014 no deps \u274c Requires Scheduler \u2705 No built-in orchestration Configuration-First Design \u2705 Declarative \u2014 code optional \ud83d\udd36 Code-first with DAGs \ud83d\udd36 Hybrid \u2014 config &amp; code \ud83d\udd36 Mostly code-based \ud83d\udd36 Code-centric \u274c Code is required (Python)"},{"location":"#roadmap-highlights","title":"Roadmap Highlights","text":"<ul> <li>\u2705 Fully battle-tested REST and GraphQL support</li> <li>\u2705 Python transform stage shipped</li> <li>\u2705 IndexedDB caching</li> <li>\ud83d\udfe0 Built-in data lineage tracking</li> <li>\ud83d\udfe0 Parallel execution in local runtime</li> </ul>"},{"location":"#get-started-today","title":"Get Started Today","text":"<p>Write once, run anywhere \u2014 from your laptop to the browser. AnkaFlow pipelines adapt to your workflow, combining flexibility, power, and portability.</p> <p>Learn more or View Examples</p>"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>Technical Summary</li> <li>Pipeline Specification</li> <li>AnkaFlow API</li> </ul>"},{"location":"#built-with","title":"Built with","text":"<p>DuckDB | YAML | Jinja</p>"},{"location":"browser-readme/","title":"\ud83e\uddea AnkaFlow in the Browser (Pyodide)","text":"<p>This guide shows how to run AnkaFlow pipelines fully in-browser, using Pyodide (Python in WebAssembly). No server, no install \u2014 pipelines run client-side using the same YAML-based definitions.</p>"},{"location":"browser-readme/#demo-options","title":"\ud83d\ude80 Demo Options","text":"Method Link \ud83e\uddea JupyterLite Notebook Launch Demo Notebook \ud83c\udf10 HTML SPA Demo Try YAML Upload Demo"},{"location":"browser-readme/#how-it-works","title":"\ud83d\udce6 How It Works","text":"<ul> <li>AnkaFlow is compatible with Pyodide (via <code>micropip</code>)</li> <li>Remote files (e.g., S3, GCS) are fetched using <code>pyodide.http.pyfetch</code> or custom implementation (e.g. <code>axios</code>)</li> <li>The SQL engine is DuckDB (running in WASM)</li> <li>Everything is local to your browser</li> </ul>"},{"location":"browser-readme/#example-jupyterlite-or-pyodide","title":"\ud83e\uddf0 Example (JupyterLite or Pyodide)","text":"<pre><code>import micropip\nawait micropip.install(\"ankaflow\")\n\nfrom ankaflow import Flow\nyaml = '''\n- name: Load\n  kind: source\n  connection:\n    kind: Parquet\n    locator: data.parquet\n\n- name: View\n  kind: transform\n  query: select * from Load\n'''\n\nFlow().run(yaml)\n</code></pre>"},{"location":"browser-readme/#notes","title":"\ud83e\udde0 Notes","text":"<ul> <li>Only packages available in the Pyodide index can be used</li> <li>Some connectors (e.g., BigQuery, ClickHouse) are not available in the browser</li> <li>All pipelines must avoid server-only dependencies when targeting Pyodide</li> </ul>"},{"location":"browser-readme/#for-developers","title":"\ud83d\udee0 For Developers","text":"<p>If you're embedding AnkaFlow in a browser app:</p> <ol> <li>Load Pyodide:</li> </ol> <pre><code>&lt;script src=\"https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js\"&gt;&lt;/script&gt;\n</code></pre> <ol> <li>Bootstrap and run:</li> </ol> <pre><code>const pyodide = await loadPyodide();\nawait pyodide.loadPackage([\"micropip\"]);\nawait pyodide.runPythonAsync(`\n    import micropip\n    await micropip.install(\"ankaflow\")\n    from ankaflow import Flow\n    Flow().run(...)\n`);\n</code></pre>"},{"location":"browser-readme/#browser-specific-modules","title":"\ud83d\udcc1 Browser-Specific Modules","text":"<p>The following modules help with Pyodide execution:</p> <ul> <li><code>connections/rest/browser.py</code>: Fetches data via <code>pyfetch</code></li> <li><code>LocalFileSystem</code>: Writes to Pyodide's <code>/tmp</code></li> <li><code>ObjectDownloader</code>: Manages cloud-to-browser downloads</li> </ul>"},{"location":"browser-readme/#works-great-in","title":"\u2705 Works Great In","text":"<ul> <li>[x] Chrome</li> <li>[x] Firefox</li> <li>[x] JupyterLite</li> <li>[x] VS Code WebView</li> <li>[x] GitHub Pages</li> </ul>"},{"location":"custom-connection/","title":"CustomConnection User Guide","text":"<p>Welcome to the CustomConnection user guide. This document will walk you through:</p> <ul> <li>What <code>CustomConnection</code> is and when to use it</li> <li>The available configuration fields</li> <li>How to reference a <code>CustomConnection</code> in your pipeline YAML</li> <li>Example usage and best practices</li> </ul>"},{"location":"custom-connection/#1-introduction","title":"1. Introduction","text":"<p><code>CustomConnection</code> allows you to plug in your own connection logic into the AnkaFlow pipeline. Your custom connection class must implement the base <code>Connection</code> interface (or derive from it), and provide the following methods:</p> <ul> <li><code>tap()</code>: Extract data from the source</li> <li><code>sink()</code>: Write data to the destination</li> <li><code>sql()</code>: Execute or generate SQL statements</li> <li><code>show_schema()</code>: Display or return the table/schema structure</li> </ul> <p>Even if your class does not need all of these for its logic, it must expose them (they can be no-ops).</p>"},{"location":"custom-connection/#2-customconnection-fields","title":"2. CustomConnection Fields","text":"<p>Below is a description of each field in the <code>CustomConnection</code> model.</p> Field Type Description <code>kind</code> <code>Literal[\"CustomConnection\"]</code> Must always be set to <code>CustomConnection</code> to select this provider. <code>module</code> <code>str</code> Python module path containing your custom connection class (e.g. <code>myapp.connectors.database</code>). <code>classname</code> <code>str</code> The name of the class to load from the specified module. <code>params</code> <code>dict</code>(default <code>{}</code>) Arbitrary parameters passed into your connection\u2019s constructor. <code>config</code> <code>ConnectionConfiguration \\| None</code> (Optional) Pre-built configuration object injected by BaseConnection super-class. <code>fields</code> <code>List[Field] \\| None</code> (Optional) Schema fields, auto-populated or used by the base implementation if needed. <code>locator</code> <code>str \\| None</code> (Optional) Name or identifier used by <code>Connection.locate()</code> for dynamic discovery."},{"location":"custom-connection/#3-defining-a-customconnection-in-your-pipeline","title":"3. Defining a CustomConnection in Your Pipeline","text":"<p>In your pipeline YAML, under a <code>sink</code> or <code>tap</code> stage, you reference a custom connection like this:</p> <pre><code>- name: MySqlWriter\n  kind: sink\n  connection:\n    kind: CustomConnection\n    module: myapp.connectors.database\n    classname: MySQL\n    params:\n      port: 5555\n</code></pre> <p>Explanation:</p> <ol> <li><code>name:</code> Logical name for the pipeline stage (<code>MySqlWriter</code>).</li> <li><code>kind: sink</code> Specifies that this stage writes data (a \"sink\" stage).</li> <li><code>connection:</code> Block configures how to connect to the target system:<ul> <li><code>kind</code>: Must be <code>CustomConnection</code>.</li> <li><code>module</code>: Python import path where your class lives.</li> <li><code>classname</code>: Actual class name inside that module.</li> <li><code>params</code>: Any keyword arguments your class expects (e.g. table name, credentials key).</li> </ul> </li> <li><code>show: 1</code> Enables schema introspection after connecting (Debug mode).</li> </ol>"},{"location":"custom-connection/#4-implementing-your-custom-class","title":"4. Implementing Your Custom Class","text":"<p>In <code>myapp/connectors/database.py</code>:</p> <pre><code>from ankaflow.connection import Connection\n\nclass MySQL(Connection):\n    def init(self):\n        # This method is provided by base class as convenience to\n        # avoid mucking with super().__init__()\n        # It is called in the end of base class __init__ hence\n        # self.config and other attributes are already populated.\n        pass\n\n    async def tap(self, query: str|None = None):\n        # Extract data from external storage and cache to pipeline\n        df = get_dataframe_from_mysql(query)\n        await self.c.register(\"tmp_{self.name}\", df)\n        await self.c.sql(f'CREATE TABLE \"{self.name}\" AS SELECT * FROM data')\n        # You can use CREATE OR REPLACE or CREATE IF NOT EXISTS\n        # CREATE OR REPLACE Will overwrite existing data (does not enforce stage name uniqueness)\n        # CREATE IF NOT EXISTS will append data if table already exists.\n        await self.c.unregister(\"tmp_{self.name}\")\n\n    async def sink(self, from_name: str):\n        # Write data to external storage\n        # from_name is always previous stage name (presumably tap or transform)\n        relation = await self.c.sql(f'SELECT * FROM \"{from_name}\"')\n        df = await relation.df() # of fetchall()\n        send_df_to_mysql(df)\n\n    async def sql(self, query: str):\n        # Execute SQL logic on target\n        execute_sql_on_mysql(query)\n\n    async def show_schema(self):\n        # Return table schema, e.g. list of Field objects\n        return []\n</code></pre> <p>Note: All methods must be async.</p> <p>Note: Your class must derive from the base <code>Connection</code> to inherit common logic and have access to <code>self.conn.cfg</code>, <code>self.conn.locator()</code>, etc.</p>"},{"location":"custom-connection/#5-best-practices-troubleshooting","title":"5. Best Practices &amp; Troubleshooting","text":"<ul> <li>Validation: Always validate your <code>params</code> against what your class expects. Mismatches will not raise errors at <code>Stages.load()</code> time but at runtime.</li> <li>Module Path: Ensure your project\u2019s root is on <code>PYTHONPATH</code> so that <code>import module</code> succeeds at runtime.</li> <li>Debugging: Use <code>show: 1</code> in your YAML to print the output of tap().</li> <li>Error Handling: Wrap your I/O in try/except blocks and surface meaningful messages; Ankalow will propagate exceptions upstream.</li> </ul> <p>Happy piping!</p>"},{"location":"getting-started/","title":"Getting Started with AnkaFlow","text":"<p>AnkaFlow is a powerful data pipeline framework that allows you to define, manage, and execute complex workflows involving data sourcing, transformation, and storage. This guide walks you through the basic steps to get started with AnkaFlow, including setting up the pipeline, installing dependencies, and running your first pipeline.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed on your machine:</p> <ol> <li>Python 3.12 or later</li> <li><code>pip</code> (Python's package installer)</li> </ol>"},{"location":"getting-started/#a-install-ankaflow","title":"A. Install AnkaFlow","text":"<p>To begin using AnkaFlow, install the required packages by running:</p> <pre><code>pip install AnkaFlow\n</code></pre> <p>In server environment you may want to include additional connectors</p> <pre><code>pip install AnkaFlow[server]\n</code></pre> <p>This will install AnkaFlow and its necessary dependencies, including <code>duckdb</code>, <code>yaml</code>, and <code>pandas</code>.</p>"},{"location":"getting-started/#b-environment-variables","title":"B. Environment variables","text":"<p>There are few environment variables that can be used to confgure AnkaFlow behaviour (usage is optional):</p> <ul> <li>Load extensions from local disk (disable network load)</li> </ul> <pre><code>export DUCKDB_EXTENSION_DIR=/my/dir\n</code></pre> <ul> <li>Disable local filesystem access</li> </ul> <pre><code>export DUCKDB_DISABLE_LOCALFS=1\n</code></pre> <ul> <li>Lock DuckDB configuration and prevent changing it in pipeline</li> </ul> <pre><code>export DUCKDB_LOCK_CONFIG=1\n</code></pre> <p>In Pyodide environment these settings are not available.</p>"},{"location":"getting-started/#1-imports","title":"1. Imports","text":"<p>To begin using AnkaFlow, you'll first need to import the necessary libraries:</p> <pre><code>from ankaflow import Flow, FlowContext, ConnectionConfiguration\nimport logging\n</code></pre>"},{"location":"getting-started/#2-create-a-logger","title":"2. Create a Logger","text":"<p>It's always good practice to create a logger for your pipeline to track its execution. The logger will help capture events, errors, and other relevant information during the pipeline's execution.</p> <pre><code># Create a logger to log pipeline events\nmy_logger = logging.getLogger('mylogger')\nmy_logger.setLevel(logging.DEBUG)\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.DEBUG)\nmy_logger.addHandler(console_handler)\n</code></pre>"},{"location":"getting-started/#3-load-yaml-from-file","title":"3. Load YAML from File","text":"<p>The configuration for your pipeline is often defined in a YAML file. This file will contain the stages of the pipeline, their configurations, and other necessary details.</p> <pre><code>from ankaflow import Stages\n\npipeline_config = Stages.load('pipeline_config.yaml')\n</code></pre> <p>This assumes you have a <code>pipeline_config.yaml</code> file in the same directory. Here's an example of what the YAML file might look like:</p> <pre><code>stages:\n  - kind: tap\n    name: source_data\n    connection:\n      kind: BigQuery\n      project_id: \"my_project\"\n      dataset: \"my_dataset\"\n  - kind: transform\n    name: process_data\n    query: \"SELECT * FROM source_data WHERE condition\"\n  - kind: sink\n    name: output_data\n    connection:\n      kind: File\n      file_path: \"output/data.csv\"\n</code></pre>"},{"location":"getting-started/#4-create-connectionconfiguration-and-flowcontext","title":"4. Create <code>ConnectionConfiguration</code> and <code>FlowContext</code>","text":"<p>The <code>ConnectionConfiguration</code> and <code>FlowContext</code> are essential for configuring the pipeline and providing context for variables and connections.</p> <pre><code># Create a ConnectionConfiguration with necessary details\nconn_config = ConnectionConfiguration(\n    kind='BigQuery', \n    project_id='my_project',\n    dataset='my_dataset'\n)\n\n# Create a FlowContext, passing any relevant configuration parameters\nflow_context = FlowContext(\n    context_variable='some_value',  # Example variable\n    connection=conn_config\n)\n</code></pre>"},{"location":"getting-started/#6-create-the-pipeline-flow","title":"6. Create the Pipeline (<code>Flow</code>)","text":"<p>Now that you have everything set up (logger, configuration, stages), you can create the <code>Flow</code> object and start running your pipeline.</p> <pre><code># Create the Flow instance with the pipeline stages, context, and configuration\nflow = Flow(\n    defs=stages, \n    context=flow_context, \n    default_connection=conn_config, \n    logger=my_logger\n)\n\n# Run the pipeline\nflow.run()\n</code></pre>"},{"location":"getting-started/#wrapping-up","title":"Wrapping Up","text":"<ul> <li>AnkaFlow allows you to create flexible and modular data pipelines by defining stages for sourcing, transforming, and storing data.</li> <li>You can manage your pipeline configuration using YAML files and easily set up connections to different data sources (BigQuery, databases, files).</li> <li>The logger provides essential insight into your pipeline\u2019s execution, helping you debug and track issues.</li> </ul> <p>Now that you've set up your first pipeline, you can customize it further by adding more stages, adjusting configurations, and experimenting with different data sources.</p>"},{"location":"getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>AnkaFlow Documentation: For more advanced usage and API references, check out the AnkaFlow documentation.</li> <li>Community Support: If you encounter any issues, join the AnkaFlow community for support and troubleshooting.</li> </ul>"},{"location":"intro/","title":"AnkaFlow","text":"<p>Write Once, Run Anywhere \u2014 SQL-powered, YAML-defined data pipelines that work in Python or the Browser.</p> <p>Run data workflows that combine Parquet, REST APIs, and SQL transforms with no infrastructure, using DuckDB under the hood. Supports local Python, Pyodide (browser), and Chrome/Firefox Extensions.</p>"},{"location":"intro/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\u2705 Run pipelines in Python or in-browser (Pyodide/WebAssembly)</li> <li>\u2705 Supports Parquet, REST, GraphQL, DeltaLake, BigQuery, ClickHouse, and more</li> <li>\u2705 No-code pipelines via YAML: source \u2192 transform \u2192 sink</li> <li>\u2705 Modular: browser/server backends, cloud optional</li> <li>\u2705 Built on DuckDB with SQL and optional Python transforms</li> <li>\u2705 Compatible with Pyodide, Chrome Extensions, and GitHub Pages</li> </ul>"},{"location":"intro/#quickstart","title":"\ud83d\udee0 Quickstart","text":""},{"location":"intro/#python-server","title":"\ud83d\udc0d Python (server)","text":"<pre><code>uv pip install -e .[server]\nankaflow pipeline.yaml\n</code></pre>"},{"location":"intro/#browser-pyodide","title":"\ud83c\udf10 Browser (Pyodide)","text":"<ol> <li>Open demo</li> <li>Upload a <code>pipeline.yaml</code> file</li> <li>View SQL output live in your browser</li> </ol>"},{"location":"intro/#dev-environment","title":"\ud83e\uddea Dev Environment","text":"<pre><code>uv pip install -e .[dev,server]\n</code></pre>"},{"location":"intro/#installation","title":"\ud83d\udce6 Installation","text":"<p>Install only the core (minimal setup with Parquet, JSON S3 support for Pyodide or remote embedding; does not include databases):</p> <pre><code>uv pip install ankaflow\n</code></pre> <p>Install with full server capabilities (BigQuery, ClickHouse, Delta write):</p> <pre><code>uv pip install ankaflow[server]\n</code></pre> <p>Install for development:</p> <pre><code>uv pip install -e .[dev,server]\n</code></pre>"},{"location":"intro/#project-layout","title":"\ud83d\udcc2 Project Layout","text":"<ul> <li><code>ankaflow/</code> \u2013 Python engine and core logic</li> <li><code>docs/</code> \u2013 Markdown and API docs (generated via pdoc)</li> </ul> <p>Upload a <code>pipeline.yaml</code> like:</p> <pre><code>- name: Load\n  kind: source\n  connection: { kind: Parquet, locator: data$*.parquet }\n\n- name: View\n  kind: transform\n  query: select * from Load\n</code></pre>"},{"location":"intro/#roadmap","title":"\ud83e\udde0 Roadmap","text":"<ul> <li>[x] Pyodide-compatible core</li> <li>[x] Remote file support (S3, GCS)</li> <li>[x] Chrome/Firefox extension support</li> <li>[ ] IndexedDB caching</li> <li>[ ] REST-to-SQL join templating</li> <li>[ ] JupyterLite integration</li> </ul>"},{"location":"intro/#contributing","title":"\ud83d\ude4c Contributing","text":"<p>PRs welcome. See CONTRIBUTING.md</p>"},{"location":"intro/#license","title":"\ud83d\udcc4 License","text":"<p>MIT License</p>"},{"location":"motherduck/","title":"Motherduck Integration Guide","text":"<p>This guide explains how to integrate and manipulate Motherduck data in your AnkaFlow pipelines. Since Motherduck requires native network access, all pipeline runs using Motherduck must execute on a server or environment with internet connectivity.</p>"},{"location":"motherduck/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>AnkaFlow installed and configured.</li> <li>Access to a Motherduck account and dataset.</li> <li>Valid Motherduck API token set as an environment variable.</li> </ul> <pre><code>export motherduck_token=your_real_token_here\n</code></pre>"},{"location":"motherduck/#2-pipeline-yaml-example","title":"2. Pipeline YAML Example","text":"<p>Below is a minimal pipeline demonstrating how to attach a Motherduck endpoint, query system tables, and retrieve sample data.</p> <pre><code>- name: First step\n  kind: self\n  query: select 42 as meaning\n\n- name: Attach Motherduck\n  kind: self\n  query: &gt;\n    attach 'md:';\n    select table_catalog, table_schema, table_name\n      from INFORMATION_SCHEMA.tables;\n  show: 5\n\n- name: Show sample data\n  kind: self\n  query: &gt;\n    select *\n      from sample_data.nyc.taxi\n      limit 10;\n  show: 5\n</code></pre>"},{"location":"motherduck/#explanation","title":"Explanation","text":"<ol> <li>API Token: AnkaFlow reads <code>motherduck_token</code> from the environment.  </li> <li><code>attach 'md:'</code>: Establishes the connection to Motherduck using the DSN prefix <code>md:</code>.  </li> <li>System Catalog Query: Lists available tables in <code>INFORMATION_SCHEMA.tables</code>.  </li> <li>Data Query: Pulls the first 10 rows from <code>sample_data.nyc.taxi</code>.</li> </ol>"},{"location":"motherduck/#3-running-the-pipeline","title":"3. Running the Pipeline","text":"<p>Execute your pipeline on a server with network access:</p> <pre><code>ankaflow pipeline.yaml\n</code></pre> <p>You should see output sections like:</p> <pre><code>---- Attach Motherduck\n  table_catalog | table_schema | table_name\n  \u2026 (5 rows shown) \u2026\n\n---- Show sample data\n  VendorID | tpep_pickup_datetime | \u2026 \n  \u2026 (10 rows shown) \u2026\n</code></pre>"},{"location":"motherduck/#4-writing-data-to-motherduck","title":"4. Writing Data to Motherduck","text":"<p>To create or overwrite tables in Motherduck, use SQL DDL statements:</p> <pre><code>- name: Create reporting table\n  kind: self\n  query: &gt;\n    attach 'md:';\n    create or replace table analytics.daily_counts as\n      select date_trunc('day', tpep_pickup_datetime) as day,\n             count(*) as rides\n        from sample_data.nyc.taxi\n       group by day;\n</code></pre> <p>Tip: Confirm write permissions and dataset names before running DDL.</p>"},{"location":"motherduck/#5-advanced-data-manipulations","title":"5. Advanced Data Manipulations","text":""},{"location":"motherduck/#51-inserting-additional-rows","title":"5.1. Inserting Additional Rows","text":"<pre><code>- name: Append special events\n  kind: self\n  query: &gt;\n    attach 'md:';\n    insert into analytics.daily_counts(day, rides)\n      values('2023-12-31', 1234);\n</code></pre>"},{"location":"motherduck/#52-updating-records","title":"5.2. Updating Records","text":"<pre><code>- name: Correct ride count\n  kind: self\n  query: &gt;\n    attach 'md:';\n    update analytics.daily_counts\n       set rides = rides + 10\n     where day = '2023-12-31';\n</code></pre>"},{"location":"motherduck/#53-cleaning-up","title":"5.3. Cleaning Up","text":"<pre><code>- name: Drop temp table\n  kind: self\n  query: &gt;\n    attach 'md:';\n    drop table if exists analytics.daily_counts_temp;\n</code></pre>"},{"location":"motherduck/#6-best-practices","title":"6. Best Practices","text":"<ul> <li>Chunk large queries to avoid timeouts.  </li> <li>Use <code>show: N</code> to preview results without overwhelming the logs.  </li> <li>Modularize repeated <code>attach 'md:'</code> calls by creating a top-level stage for connection.  </li> <li>Secure your token: do not commit <code>motherduck_token</code> to source control.</li> </ul>"},{"location":"motherduck/#7-troubleshooting","title":"7. Troubleshooting","text":"<ul> <li>Authentication errors \u2192 Verify <code>motherduck_token</code> is correct and exported.  </li> <li>Network failures \u2192 Ensure outbound connectivity to Motherduck endpoints.  </li> <li>Permission denied \u2192 Check your account\u2019s dataset ACLs.</li> </ul> <p>Happy querying with Motherduck in AnkaFlow!</p>"},{"location":"overview/","title":"Technical Introduction Document: Setting Up and Configuring a Data Pipeline in AnkaFlow","text":"<p>This document provides a step-by-step guide for setting up and configuring a data pipeline using AnkaFlow. The pipeline framework facilitates seamless data transformations and efficient management of stages such as data sourcing (tap), transformations, and sink operations.</p>"},{"location":"overview/#1-overview-of-the-pipeline-system","title":"1. Overview of the Pipeline System","text":"<p>A pipeline in AnkaFlow defines the flow of data through various stages. Each stage corresponds to a specific data operation such as:</p> <ul> <li>Tap Stage: Sources data from a remote or local system.</li> <li>Transform Stage: Processes or transforms data, typically using SQL queries or other computational logic.</li> <li>Sink Stage: Stores the processed data into a target system (e.g., a database, file system, or cloud storage).</li> </ul> <p>These stages are executed sequentially, with each stage building on the data produced by the previous one.</p>"},{"location":"overview/#2-pipeline-components","title":"2. Pipeline Components","text":"<ul> <li>Flow: The primary object that controls the execution of the pipeline. It defines the order of stages, manages connections, and handles error flow control.</li> </ul> <p>Example Initialization:   <code>python   flow = Flow(       defs=stages,        context=flowuct_context,        default_connection=conn_config,        logger=my_logger   )</code></p> <ul> <li><code>defs</code>: A list of stages (e.g., <code>TapStage</code>, <code>TransformStage</code>, <code>SinkStage</code>).</li> <li><code>context</code>: The dynamic context used in query templates.</li> <li><code>default_connection</code>: Connection configuration passed to the underlying systems.</li> <li> <p><code>logger</code>: Optional logger for logging pipeline activities.</p> </li> <li> <p>Datablock: Represents an executable piece of the pipeline. Each <code>Datablock</code> corresponds to a specific stage and includes the logic to execute that stage, e.g., reading data, transforming it, or storing it.</p> </li> </ul> <p>Example:   <code>python   datablock = Datablock(       conn=db_connection,       defs=datablock_def,       context=flow_context,       default_connection=conn_config   )</code></p> <ul> <li>Stage Handler: Each stage (tap, transform, sink) is associated with a handler that defines how the stage should be executed. The handler interacts with the data and performs the necessary operations.</li> </ul>"},{"location":"overview/#3-configuration-and-setup","title":"3. Configuration and Setup","text":"<p>To configure a pipeline, you must define the following:</p> <ul> <li>Stages: Define the sequence of operations your pipeline will execute. Each stage must specify its type (<code>tap</code>, <code>transform</code>, <code>sink</code>) and the corresponding logic (e.g., queries, data manipulations).</li> <li>Connection Configurations: Each stage typically connects to a data source or target system. Connection details, such as credentials, endpoint URLs, and database configurations, are passed into the stages.</li> </ul> <p>Example:   <code>yaml   stages:     - kind: tap       name: source_data       connection:         kind: BigQuery         project_id: \"my_project\"         dataset: \"my_dataset\"     - kind: transform       name: process_data       query: \"SELECT * FROM source_data WHERE condition\"     - kind: sink       name: output_data       connection:         kind: File         file_path: \"output/data.csv\"</code></p> <ul> <li>Variables: If your pipeline stages reference dynamic values, such as dates or keys, you can define these variables within the <code>context</code> to be injected into your queries at runtime.</li> </ul>"},{"location":"overview/#4-executing-the-pipeline","title":"4. Executing the Pipeline","text":"<p>Once the pipeline is defined, you can execute it using the <code>Flow</code> class. This will initiate the stages in sequence and handle the transformation of data across all stages.</p> <ul> <li> <p>Run Pipeline: Execute the pipeline and get the final data.   <code>python   flow.run()</code></p> </li> <li> <p>Access Output Data: Once the pipeline runs, you can access the data produced by the final stage via the <code>df()</code> method.   <code>python   result_df = flow.df()</code></p> </li> </ul>"},{"location":"overview/#5-error-handling-and-flow-control","title":"5. Error Handling and Flow Control","text":"<p>AnkaFlow provides robust error handling mechanisms, ensuring that errors are managed appropriately during pipeline execution. The <code>FlowControl</code> configuration allows you to define how errors should be handled (e.g., fail or warn).</p> <p>Example of flow control:</p> <pre><code>flow_control = FlowControl(on_error=\"fail\")\n</code></pre>"},{"location":"overview/#6-show-schema-for-stages","title":"6. Show Schema for Stages","text":"<p>Each stage may expose a schema that defines the structure of the data. This can be useful to inspect and verify the data format before moving to subsequent stages.</p> <pre><code>schema = flow.show_schema()\n</code></pre>"},{"location":"overview/#7-testing-and-debugging-pipelines","title":"7. Testing and Debugging Pipelines","text":"<p>For reliable pipeline execution, it is important to test and debug each stage. AnkaFlow includes utilities for mock testing, simulating remote connections, and verifying the correctness of SQL queries.</p> <ul> <li>Unit Tests: Each stage can be tested in isolation. For example, the <code>TapStageHandler</code> can be tested to ensure that it retrieves the correct data from the source.</li> </ul>"},{"location":"overview/#8-conclusion","title":"8. Conclusion","text":"<p>With these components and configurations, AnkaFlow allows you to define, execute, and manage data pipelines flexibly. It integrates various data sources and sinks, applies transformations, and enables efficient error handling, all while keeping the pipeline definitions clean and reusable.</p> <p>Next Steps: - Customize your pipeline based on the specific sources, transformations, and sinks relevant to your use case. - Explore advanced features like parallel pipeline execution and nested sub-pipelines for more complex workflows.</p> <p>This document serves as an introduction to configuring a basic pipeline setup in AnkaFlow. For more advanced configurations and features, refer to the detailed documentation on stages, handlers, and connections.</p>"},{"location":"api/","title":"Index","text":"<p>API areas</p>"},{"location":"api/ankaflow/","title":"AnkaFlow","text":""},{"location":"api/ankaflow/#ankaflow.__all__","title":"__all__","text":"<pre><code>__all__ = [\n    \"Flow\",\n    \"AsyncFlow\",\n    \"FlowControl\",\n    \"Stages\",\n    \"ConnectionConfiguration\",\n    \"Variables\",\n    \"FlowContext\",\n    \"FlowRunError\",\n    \"FlowError\",\n    \"S3Config\",\n    \"GSConfig\",\n    \"ClickhouseConfig\",\n    \"BigQueryConfig\",\n    \"BucketConfig\",\n]\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow","title":"AsyncFlow","text":"<pre><code>AsyncFlow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DDB = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Controls the flow of data based on the pipeline definition.</p> <p>The stages are run in the defined order and can either fetch data from a source (tap), transform via SQL, or store to a sink.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>Dynamic context for query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database.</p> required <code>variables</code> <code>Variables</code> <p>Variables for tap or prepare. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger for show() requests. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DDB</code> <p>Existing DuckDB connection. If not set, a new connection is created.</p> <code>None</code> <code>flow_control</code> <code>FlowControl</code> <p>Flow control configuration. Defaults to FlowControl().</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Log context passed to each stage. Defaults to None.</p> <code>None</code>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.log","title":"log","text":"<pre><code>log: Logger\n</code></pre> <p>Returns the logger.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.logs","title":"logs","text":"<pre><code>logs: List[str]\n</code></pre> <p>Returns the log buffer.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connects to the underlying database if not already connected.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns a dataframe from the last stage.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull the final dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.run","title":"run","text":"<pre><code>run() -&gt; AsyncFlow\n</code></pre> <p>Runs the pipeline stages.</p> <p>Returns:</p> Name Type Description <code>AsyncFlow</code> <code>AsyncFlow</code> <p>Self instance after running the pipeline.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas for all supported stages.</p> <p>Returns:</p> Type Description <code>List[SchemaItem]</code> <p>t.List[m.SchemaItem]: List of schema items.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow/#ankaflow.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>ClickHouse database configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.Flow","title":"Flow","text":"<pre><code>Flow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Flow controls the flow of data based on the pipeline definition. The stages are run in the defined order and can either fetch data from source (tap), transform via SQL statement, or store to sink.</p> <p>This is the sync version and should be used in most cases.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>dynamic context that can be used in query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database (s3 bucket, database connection string &amp;c).</p> required <code>variables</code> <code>dict</code> <p>Any variables that can be referenced by a tap or prepare. Ony data structures that can be passed to pl.from_dicts() are supported in taps. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then show() requests are logged. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DuckDBPyConnection</code> <p>Existing DuckDB connection. If not set new connection will be created.</p> <code>None</code>"},{"location":"api/ankaflow/#ankaflow.Flow.df","title":"df","text":"<pre><code>df()\n</code></pre> <p>Returns dataframe from the last stage</p>"},{"location":"api/ankaflow/#ankaflow.Flow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull final dataframe</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: data from last stage</p>"},{"location":"api/ankaflow/#ankaflow.Flow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas of all supported stages</p>"},{"location":"api/ankaflow/#ankaflow.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow/#ankaflow.FlowControl","title":"FlowControl","text":"<pre><code>FlowControl(on_error: str = 'fail')\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.FlowError","title":"FlowError","text":"<p>               Bases: <code>Exception</code></p>"},{"location":"api/ankaflow/#ankaflow.FlowRunError","title":"FlowRunError","text":"<p>               Bases: <code>FlowError</code></p>"},{"location":"api/ankaflow/#ankaflow.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow/#ankaflow.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow/#ankaflow.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow/#ankaflow.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Datablock]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow/#ankaflow.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Datablock]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Datablock]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p>"},{"location":"api/ankaflow/#ankaflow.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p>"},{"location":"api/ankaflow/#ankaflow.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Datablock]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Datablock]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Datablock]</code> <p>from first to last.</p>"},{"location":"api/ankaflow/#ankaflow.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"},{"location":"api/ankaflow/#ankaflow.api","title":"api","text":""},{"location":"api/ankaflow/#ankaflow.api.API","title":"API","text":""},{"location":"api/ankaflow/#ankaflow.api.API.dt","title":"dt","text":"<pre><code>dt(\n    datelike: Union[\n        str, int, float, date, datetime, Arrow\n    ] = None,\n    tz: str = None,\n    format: str = None,\n    default: str = None,\n)\n</code></pre> <p>Attempts to convert input object into Arrow.</p> <p>Parameters:</p> Name Type Description Default <code>tz</code> <code>str</code> <p>IANA timezone string If . Defaults to None.</p> <code>None</code> <code>format</code> <code>str</code> <p>If set use this format to parse input. Defaults to None.</p> <code>None</code> <code>default</code> <code>str</code> <p>Use default timestamp if first attempt fails. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Arrow</code> <p>Arrow object</p>"},{"location":"api/ankaflow/#ankaflow.api.API.error","title":"error","text":"<pre><code>error(expression: Any, message: str) -&gt; Any\n</code></pre> <p>Raises an exception if input expression evaluates to True.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Any</code> <p>Expression to be evaluetd</p> required <code>message</code> <code>str</code> <p>Message to include in exception</p> required <p>Raises:</p> Type Description <code>UserGeneratedError</code> <p>description</p> <p>Returns:</p> Type Description <code>Any</code> <p>t.Any: Expression</p>"},{"location":"api/ankaflow/#ankaflow.api.API.look","title":"look","text":"<pre><code>look(\n    lookup: str, data: Any, default: Any = None\n) -&gt; Union[str, None]\n</code></pre> <p>Extracts value from given structure, or default value if requested value not found.</p> <p>Parameters:</p> Name Type Description Default <code>lookup</code> <code>str</code> <p>Lookup query</p> required <code>data</code> <code>Any</code> <p>Iterable object</p> required <code>default</code> <code>Any</code> <p>Default value to return. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: description</p>"},{"location":"api/ankaflow/#ankaflow.api.API.peek","title":"peek","text":"<pre><code>peek(value: Any) -&gt; str\n</code></pre> <p>Returns information about value type: module.Class Useful for debugging data obtained from remote sources.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Any value</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p>"},{"location":"api/ankaflow/#ankaflow.api.API.setvariable","title":"setvariable","text":"<pre><code>setvariable(collection: dict, key: str, value: Any) -&gt; Any\n</code></pre> <p>Attempts to assign value to a dictionary under the given key.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>dict</code> <p>Dictionary to use e.g. variables</p> required <code>key</code> <code>str</code> <p>Key to store thalue to be storese value under</p> required <code>value</code> <code>Any</code> <p>V</p> required <p>Returns:</p> Type Description <code>Any</code> <p>t.Any: Original value</p>"},{"location":"api/ankaflow/#ankaflow.api.API.sqltuple","title":"sqltuple","text":"<pre><code>sqltuple(iterable: Iterable, mode: str = 'string')\n</code></pre> <p>Returns SQL tuple literal from given iterable.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>Any iterable.</p> required <code>mode</code> <code>str</code> <p>\"string\"|\"number\". Defaults to \"string\".</p> <code>'string'</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When unsupported mode is used</p> <p>Returns:</p> Name Type Description <code>str</code> <p>SQL tuple literal</p>"},{"location":"api/ankaflow/#ankaflow.cli","title":"cli","text":""},{"location":"api/ankaflow/#ankaflow.cli.resolve_yaml_path","title":"resolve_yaml_path","text":"<pre><code>resolve_yaml_path(path_arg: str) -&gt; Path\n</code></pre> <p>Resolve YAML path, remapping 'DEMO' to a relative demo file path.</p>"},{"location":"api/ankaflow/#ankaflow.connections","title":"connections","text":""},{"location":"api/ankaflow/#ankaflow.connections.load_connection","title":"load_connection","text":"<pre><code>load_connection(\n    cls: Type[\n        Union[Connection, Rest, CustomConnection, Dimension]\n    ],\n) -&gt; Type[Connection]\n</code></pre> <p>Load built-in connection or custom connection from module</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Connection name</p> required <code>module</code> <code>str</code> <p>If set then try to load from given module. Defaults to None i.e. built-in connection.</p> required"},{"location":"api/ankaflow/#ankaflow.connections.bigquery","title":"bigquery","text":""},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery","title":"BigQuery","text":"<pre><code>BigQuery(\n    duck: DDB,\n    name: str,\n    connection: Connection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset()\n</code></pre> <p>Creates dataset if it does not exist.</p>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.init","title":"init","text":"<pre><code>init()\n</code></pre> <p>Initializes internal configs and client.</p>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.locate","title":"locate","text":"<pre><code>locate(\n    name: Optional[str] = None, use_wildcard: bool = False\n) -&gt; str\n</code></pre> <p>Returns the fully qualified BigQuery table name using dot notation.</p>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; Fields\n</code></pre> <p>Infers runtime schema from BigQuery data (via DuckDB) and returns as Fields.</p>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.sink","title":"sink","text":"<pre><code>sink(from_name: str)\n</code></pre> <p>Public entrypoint for writing DuckDB data to BigQuery.</p>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.sql","title":"sql","text":"<pre><code>sql(statement: str) -&gt; Any\n</code></pre> <p>Executes a BigQuery SQL statement and logs the result.</p>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.tap","title":"tap","text":"<pre><code>tap(query: Optional[str] = None, limit: int = 0)\n</code></pre> <p>Executes a BigQuery query and loads result into DuckDB.</p>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse","title":"clickhouse","text":""},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.Clickhouse","title":"Clickhouse","text":"<pre><code>Clickhouse(\n    duck: DDB,\n    name: str,\n    connection: Connection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.Clickhouse.init","title":"init","text":"<pre><code>init() -&gt; None\n</code></pre> <p>Initializes connection-specific defaults.</p>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.Clickhouse.locate","title":"locate","text":"<pre><code>locate(\n    name: Optional[str] = None, use_wildcard: bool = False\n) -&gt; str\n</code></pre> <p>Returns the fully-qualified table reference with validation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Table name override (unused).</p> <code>None</code> <code>use_wildcard</code> <code>bool</code> <p>Placeholder flag (unused).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Fully-qualified table name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If locator format is invalid.</p>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient","title":"ClickhouseClient","text":"<pre><code>ClickhouseClient(cfg: ConnectionConfiguration)\n</code></pre> <p>Initializes a Clickhouse client wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ConnectionConfiguration</code> <p>Connection configuration.</p> required"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.connect","title":"connect","text":"<pre><code>connect() -&gt; Iterator[Client]\n</code></pre> <p>Context-managed connection that disconnects cleanly.</p>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.insert_dataframe","title":"insert_dataframe","text":"<pre><code>insert_dataframe(\n    client: Client, query: str, df: DataFrame\n) -&gt; Any\n</code></pre> <p>Inserts a DataFrame into Clickhouse using the given query.</p>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.query_df","title":"query_df","text":"<pre><code>query_df(client: Client, query: str) -&gt; DataFrame\n</code></pre> <p>Executes a SELECT query and returns a DataFrame.</p>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.stream_query","title":"stream_query","text":"<pre><code>stream_query(\n    client: Client,\n    query: str,\n    block_size: int | None = None,\n) -&gt; Generator\n</code></pre> <p>Streams ClickHouse query results.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A live Clickhouse connection.</p> required <code>query</code> <code>str</code> <p>SQL query.</p> required <code>block_size</code> <code>int</code> <p>Optional server-side row block size.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator</code> <p>row blocks from ClickHouse.</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection","title":"connection","text":""},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection","title":"Connection","text":"<pre><code>Connection(\n    duck: DDB,\n    name: str,\n    connection: Connection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>Base class for connections such as Deltatable or parquet.</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.init","title":"init","text":"<pre><code>init()\n</code></pre> <p>Additional initialization called from within init.</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.ranking","title":"ranking","text":"<pre><code>ranking(\n    selectable: str, query: str, validate_simple=False\n) -&gt; tuple[str, str]\n</code></pre> <p>Injects row ranking logic to deduplicate records based on versioning.</p> <p>If the connection defines a <code>version</code> and <code>key</code>, modifies the query to include a <code>ROW_NUMBER() OVER (...) AS __rank__</code> column and wraps it in a subquery that can be filtered using <code>WHERE __rank__ = 1</code>.</p> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: Transformed SQL query and the WHERE clause if applicable.</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.sink","title":"sink","text":"<pre><code>sink(from_name: str)\n</code></pre> <p>Sink or store data from given <code>name</code> (typically previous stage)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>string</code> <p>table name to use as source</p> required"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.sql","title":"sql","text":"<pre><code>sql(statement: str) -&gt; Any\n</code></pre> <p>Execute raw sql using the specified connection (if supported).</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.tap","title":"tap","text":"<pre><code>tap(query: Optional[str] = None, limit: int = 0)\n</code></pre> <p>Implements loading from source storage.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Reduce the number of rows returned. Defaults to 0.</p> <code>0</code>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Locator","title":"Locator","text":"<pre><code>Locator(config: ConnectionConfiguration)\n</code></pre> <p>Initializes the Locator with connection configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConnectionConfiguration</code> <p>Config with bucket, prefix, etc.</p> required"},{"location":"api/ankaflow/#ankaflow.connections.connection.Locator.locate","title":"locate","text":"<pre><code>locate(name: str, use_wildcard: bool = False) -&gt; CommonPath\n</code></pre> <p>Resolves a full path using bucket, prefix, and optional wildcard substitution.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Relative or absolute path string.</p> required <code>use_wildcard</code> <code>bool</code> <p>Whether to apply wildcard regex substitution.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>CommonPath</code> <code>CommonPath</code> <p>A fully resolved path.</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Schema","title":"Schema","text":"<pre><code>Schema(duck: DDB)\n</code></pre> <p>Class for working with table schemas</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Schema.generate","title":"generate","text":"<pre><code>generate(\n    table: str, fields: Fields, exists_ok: bool = False\n) -&gt; str\n</code></pre> <p>Generates CREATE TABLE statement from schema.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name to create</p> required <code>schema</code> <code>Fields</code> <p>List of fields (names and dtypes)</p> required <code>exists_ok</code> <code>bool</code> <p>If true then create statement includes <code>IF NOT EXISTS</code>. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>CREATE statement</p>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Schema.show","title":"show","text":"<pre><code>show(table: str) -&gt; Fields\n</code></pre> <p>Returns the schema of a table as a validated Fields model.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The name of the DuckDB table/view.</p> required <p>Returns:</p> Type Description <code>Fields</code> <p>m.Fields: List of validated fields.</p>"},{"location":"api/ankaflow/#ankaflow.connections.delta","title":"delta","text":""},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable","title":"Deltatable","text":"<pre><code>Deltatable(\n    duck: DDB,\n    name: str,\n    connection: Connection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable.sink","title":"sink","text":"<pre><code>sink(from_name: str)\n</code></pre> <p>DeltaTable sink behavior is driven by available schema and data:</p>"},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable.sink--strategy-matrix","title":"Strategy Matrix:","text":"Schema (conn.fields) Data Available Resulting Strategy No No SKIP \u2192 no action Yes No CREATE \u2192 define schema Yes Yes WRITE \u2192 create &amp; write No Yes WRITE \u2192 infer &amp; write -------------------------------------------------------------------- <p>When table already exists (is_deltatable): - CREATE \u2192 skip creation, write only if data present - WRITE  \u2192 directly append/merge data to table</p> <p>Optimizations (vacuum/compact) applied if self.conn.optimize is set.</p>"},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable.sql","title":"sql","text":"<pre><code>sql(statement: str)\n</code></pre> <p>Executes a limited subset of SQL-like operations on a Delta table.</p> <p>Supported commands: - DROP Deltatable - TRUNCATE Deltatable</p> <p>All other statements raise an error.</p>"},{"location":"api/ankaflow/#ankaflow.connections.errors","title":"errors","text":""},{"location":"api/ankaflow/#ankaflow.connections.errors.ConnectionException","title":"ConnectionException","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for connection errors with optional message redaction.</p>"},{"location":"api/ankaflow/#ankaflow.const","title":"const","text":""},{"location":"api/ankaflow/#ankaflow.core","title":"core","text":""},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow","title":"AsyncFlow","text":"<pre><code>AsyncFlow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DDB = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Controls the flow of data based on the pipeline definition.</p> <p>The stages are run in the defined order and can either fetch data from a source (tap), transform via SQL, or store to a sink.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>Dynamic context for query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database.</p> required <code>variables</code> <code>Variables</code> <p>Variables for tap or prepare. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger for show() requests. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DDB</code> <p>Existing DuckDB connection. If not set, a new connection is created.</p> <code>None</code> <code>flow_control</code> <code>FlowControl</code> <p>Flow control configuration. Defaults to FlowControl().</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Log context passed to each stage. Defaults to None.</p> <code>None</code>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.log","title":"log","text":"<pre><code>log: Logger\n</code></pre> <p>Returns the logger.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.logs","title":"logs","text":"<pre><code>logs: List[str]\n</code></pre> <p>Returns the log buffer.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connects to the underlying database if not already connected.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns a dataframe from the last stage.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull the final dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.run","title":"run","text":"<pre><code>run() -&gt; AsyncFlow\n</code></pre> <p>Runs the pipeline stages.</p> <p>Returns:</p> Name Type Description <code>AsyncFlow</code> <code>AsyncFlow</code> <p>Self instance after running the pipeline.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas for all supported stages.</p> <p>Returns:</p> Type Description <code>List[SchemaItem]</code> <p>t.List[m.SchemaItem]: List of schema items.</p>"},{"location":"api/ankaflow/#ankaflow.core.Flow","title":"Flow","text":"<pre><code>Flow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Flow controls the flow of data based on the pipeline definition. The stages are run in the defined order and can either fetch data from source (tap), transform via SQL statement, or store to sink.</p> <p>This is the sync version and should be used in most cases.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>dynamic context that can be used in query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database (s3 bucket, database connection string &amp;c).</p> required <code>variables</code> <code>dict</code> <p>Any variables that can be referenced by a tap or prepare. Ony data structures that can be passed to pl.from_dicts() are supported in taps. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then show() requests are logged. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DuckDBPyConnection</code> <p>Existing DuckDB connection. If not set new connection will be created.</p> <code>None</code>"},{"location":"api/ankaflow/#ankaflow.core.Flow.df","title":"df","text":"<pre><code>df()\n</code></pre> <p>Returns dataframe from the last stage</p>"},{"location":"api/ankaflow/#ankaflow.core.Flow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull final dataframe</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: data from last stage</p>"},{"location":"api/ankaflow/#ankaflow.core.Flow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas of all supported stages</p>"},{"location":"api/ankaflow/#ankaflow.core.flow","title":"flow","text":""},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow","title":"AsyncFlow","text":"<pre><code>AsyncFlow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DDB = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Controls the flow of data based on the pipeline definition.</p> <p>The stages are run in the defined order and can either fetch data from a source (tap), transform via SQL, or store to a sink.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>Dynamic context for query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database.</p> required <code>variables</code> <code>Variables</code> <p>Variables for tap or prepare. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger for show() requests. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DDB</code> <p>Existing DuckDB connection. If not set, a new connection is created.</p> <code>None</code> <code>flow_control</code> <code>FlowControl</code> <p>Flow control configuration. Defaults to FlowControl().</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Log context passed to each stage. Defaults to None.</p> <code>None</code>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.log","title":"log","text":"<pre><code>log: Logger\n</code></pre> <p>Returns the logger.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.logs","title":"logs","text":"<pre><code>logs: List[str]\n</code></pre> <p>Returns the log buffer.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connects to the underlying database if not already connected.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns a dataframe from the last stage.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull the final dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.run","title":"run","text":"<pre><code>run() -&gt; AsyncFlow\n</code></pre> <p>Runs the pipeline stages.</p> <p>Returns:</p> Name Type Description <code>AsyncFlow</code> <code>AsyncFlow</code> <p>Self instance after running the pipeline.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas for all supported stages.</p> <p>Returns:</p> Type Description <code>List[SchemaItem]</code> <p>t.List[m.SchemaItem]: List of schema items.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.BaseStageHandler","title":"BaseStageHandler","text":"<pre><code>BaseStageHandler(datablock: Datablock)\n</code></pre> <p>Abstract base class for stage execution handlers.</p> <p>Attributes:</p> Name Type Description <code>datablock</code> <code>Datablock</code> <p>The datablock instance holding execution context.</p> <p>Parameters:</p> Name Type Description Default <code>datablock</code> <code>Datablock</code> <p>The datablock to be executed.</p> required"},{"location":"api/ankaflow/#ankaflow.core.flow.BaseStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes the stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Result of the stage execution.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.Datablock","title":"Datablock","text":"<pre><code>Datablock(\n    conn: DDB,\n    defs: Datablock,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Optional[Variables] = None,\n    logger: Optional[Logger] = None,\n    prevous_stage: Optional[str] = None,\n    log_context: Optional[str] = None,\n)\n</code></pre> <p>Executable piece of a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DDB</code> <p>DuckDB connection.</p> required <code>defs</code> <code>Datablock</code> <p>Datablock definition.</p> required <code>context</code> <code>FlowContext</code> <p>Context object.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Global persistent connection configuration.</p> required <code>variables</code> <code>Variables</code> <p>Any variables passed to pipeline. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then messages are sent to this logger. Defaults to None.</p> <code>None</code> <code>prevous_stage</code> <code>str</code> <p>Reference to previous stage, used in sink to obtain data from. Defaults to None.</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Additional logging context. Defaults to None.</p> <code>None</code>"},{"location":"api/ankaflow/#ankaflow.core.flow.Datablock.do","title":"do","text":"<pre><code>do() -&gt; Union[str, None]\n</code></pre> <p>Executes the current block.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Executed block name.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.Datablock.prepare","title":"prepare","text":"<pre><code>prepare(src: Datablock) -&gt; Datablock\n</code></pre> <p>Prepare dynamic variables in the block.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Datablock</code> <p>Current block.</p> required <p>Returns:</p> Type Description <code>Datablock</code> <p>m.Datablock: Block with variables evaluated.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.Datablock.render","title":"render","text":"<pre><code>render(templ: Union[str, BaseModel]) -&gt; str\n</code></pre> <p>Evaluates a template string using context and API.</p> Available variables in the template <ul> <li>context: Context object.</li> <li>API: API object.</li> <li>variables: Variables dictionary.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>templ</code> <code>Union[str, BaseModel]</code> <p>Template containing variables.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Evaluated query string.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow","title":"Flow","text":"<pre><code>Flow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Flow controls the flow of data based on the pipeline definition. The stages are run in the defined order and can either fetch data from source (tap), transform via SQL statement, or store to sink.</p> <p>This is the sync version and should be used in most cases.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>dynamic context that can be used in query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database (s3 bucket, database connection string &amp;c).</p> required <code>variables</code> <code>dict</code> <p>Any variables that can be referenced by a tap or prepare. Ony data structures that can be passed to pl.from_dicts() are supported in taps. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then show() requests are logged. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DuckDBPyConnection</code> <p>Existing DuckDB connection. If not set new connection will be created.</p> <code>None</code>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow.df","title":"df","text":"<pre><code>df()\n</code></pre> <p>Returns dataframe from the last stage</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull final dataframe</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: data from last stage</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas of all supported stages</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.InternalStageHandler","title":"InternalStageHandler","text":"<pre><code>InternalStageHandler(datablock: Datablock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for internal stages.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.InternalStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes an internal stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the created table.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.PipelineStageHandler","title":"PipelineStageHandler","text":"<pre><code>PipelineStageHandler(datablock: Datablock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for pipeline stages, including loop implementation.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.PipelineStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a pipeline stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Result of the pipeline stage execution.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.SQLStageHandler","title":"SQLStageHandler","text":"<pre><code>SQLStageHandler(datablock: Datablock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for SQL stages.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.SQLStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes an SQL stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the target object.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.SinkStageHandler","title":"SinkStageHandler","text":"<pre><code>SinkStageHandler(datablock: Datablock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for sink stages.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.SinkStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a sink stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: None.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageFactory","title":"StageFactory","text":"<p>Factory to obtain the appropriate stage handler based on stage kind.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageFactory.get_handler","title":"get_handler","text":"<pre><code>get_handler(datablock: Datablock) -&gt; BaseStageHandler\n</code></pre> <p>Creates a stage handler instance.</p> <p>Parameters:</p> Name Type Description Default <code>datablock</code> <code>Datablock</code> <p>The datablock instance.</p> required <p>Returns:</p> Name Type Description <code>BaseStageHandler</code> <code>BaseStageHandler</code> <p>The corresponding stage handler.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.TapStageHandler","title":"TapStageHandler","text":"<pre><code>TapStageHandler(datablock: Datablock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for source stages.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.TapStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a source stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the created source table.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.TransformStageHandler","title":"TransformStageHandler","text":"<pre><code>TransformStageHandler(datablock: Datablock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for transform stages.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.TransformStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a transform stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the created view.</p>"},{"location":"api/ankaflow/#ankaflow.errors","title":"errors","text":""},{"location":"api/ankaflow/#ankaflow.internal","title":"internal","text":""},{"location":"api/ankaflow/#ankaflow.internal.browser","title":"browser","text":""},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB","title":"DDB","text":"<pre><code>DDB(\n    connection_options: ConnectionConfiguration,\n    persisted: Optional[str] = None,\n)\n</code></pre> <p>Initializes the DuckDB browser-bound client.</p> <p>Parameters:</p> Name Type Description Default <code>connection_options</code> <code>ConnectionConfiguration</code> <p>Connection settings.</p> required"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.connect","title":"connect","text":"<pre><code>connect() -&gt; DDB\n</code></pre> <p>Establishes a DuckDB connection and initializes context.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.delta_scan","title":"delta_scan","text":"<pre><code>delta_scan()\n</code></pre> <p>Stub for unsupported delta_scan.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.get","title":"get","text":"<pre><code>get() -&gt; DuckDBPyConnection\n</code></pre> <p>Returns the internal DuckDB connection object.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.inject_secrets","title":"inject_secrets","text":"<pre><code>inject_secrets(\n    name: str, connection_options: ConnectionConfiguration\n)\n</code></pre> <p>Injects secret configuration dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Secret identifier.</p> required <code>connection_options</code> <code>ConnectionConfiguration</code> <p>Configuration details.</p> required"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.parquet_scan","title":"parquet_scan","text":"<pre><code>parquet_scan()\n</code></pre> <p>Stub for unsupported parquet_scan.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    data: str, table: str, read_opts: dict\n) -&gt; DuckDBPyRelation\n</code></pre> <p>Reads CSV data into a DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>File path or URL.</p> required <code>table</code> <code>str</code> <p>Target table name.</p> required <code>read_opts</code> <code>dict</code> <p>Options passed to DuckDB read_csv().</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>duckdb.DuckDBPyRelation: The resulting table.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.read_json","title":"read_json","text":"<pre><code>read_json(\n    data: str, table: str, read_opts: dict\n) -&gt; DuckDBPyRelation\n</code></pre> <p>Reads JSON data into a DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>File path or URL.</p> required <code>table</code> <code>str</code> <p>Target table name.</p> required <code>read_opts</code> <code>dict</code> <p>Options passed to DuckDB read_json().</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>duckdb.DuckDBPyRelation: The resulting table.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.read_parquet","title":"read_parquet","text":"<pre><code>read_parquet(\n    data: str, table: str, read_opts: dict\n) -&gt; DuckDBPyRelation\n</code></pre> <p>Reads Parquet data into a DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>File path or URL.</p> required <code>table</code> <code>str</code> <p>Target table name.</p> required <code>read_opts</code> <code>dict</code> <p>Options passed to DuckDB read_parquet().</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>duckdb.DuckDBPyRelation: The resulting table.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.register","title":"register","text":"<pre><code>register(view_name: str, object)\n</code></pre> <p>Registers a Python object as a DuckDB view.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>View name.</p> required <code>object</code> <p>Python object to register.</p> required"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.sql","title":"sql","text":"<pre><code>sql(query: str) -&gt; Relation\n</code></pre> <p>Executes a DuckDB SQL query after intercepting remote I/O references.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL string.</p> required <p>Returns:</p> Name Type Description <code>Relation</code> <code>Relation</code> <p>Resulting relation object.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.unregister","title":"unregister","text":"<pre><code>unregister(view_name: str)\n</code></pre> <p>Unregisters a DuckDB view.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>View name to remove.</p> required"},{"location":"api/ankaflow/#ankaflow.internal.browser.DuckDBIORewriter","title":"DuckDBIORewriter","text":"<pre><code>DuckDBIORewriter(\n    remote: RemoteObject, filesystem: FileSystem\n)\n</code></pre> <p>Intercepts and rewrites DuckDB read_*() calls that reference remote paths.</p> <p>Parameters:</p> Name Type Description Default <code>scope</code> <code>str</code> <p>scope of the current rewrite (stage name)</p> required <code>remote</code> <code>RemoteObject</code> <p>Remote downloader/uploader.</p> required <code>filesystem</code> <code>FileSystem</code> <p>Local filesystem to store downloaded content.</p> required"},{"location":"api/ankaflow/#ankaflow.internal.browser.DuckDBIORewriter.rewrite","title":"rewrite","text":"<pre><code>rewrite(query: str) -&gt; str\n</code></pre> <p>Rewrites read_*() calls with remote URIs to local paths.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rewritten query.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>For globs or multi-file remote reads.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.Relation","title":"Relation","text":"<pre><code>Relation(relation: DuckDBPyRelation)\n</code></pre> <p>Wraps a DuckDBPyRelation for async compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>relation</code> <code>DuckDBPyRelation</code> <p>DuckDB relation object.</p> required"},{"location":"api/ankaflow/#ankaflow.internal.browser.Relation.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns the relation as a pandas DataFrame.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.RemoteObject","title":"RemoteObject","text":"<pre><code>RemoteObject(secrets: dict, fs: FileSystem)\n</code></pre> <p>Initializes a RemoteObject handler.</p> <p>Parameters:</p> Name Type Description Default <code>secrets</code> <code>dict</code> <p>Credentials or connection metadata.</p> required <code>fs</code> <code>FileSystem</code> <p>File system handler to save fetched content.</p> required"},{"location":"api/ankaflow/#ankaflow.internal.browser.RemoteObject.fetch","title":"fetch","text":"<pre><code>fetch(remote_path: RemotePath) -&gt; bytes\n</code></pre> <p>Downloads the content at the remote path.</p> <p>Parameters:</p> Name Type Description Default <code>remote_path</code> <code>RemotePath</code> <p>The remote object URI.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Content fetched from the remote URL.</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If fetch fails due to network or CORS.</p>"},{"location":"api/ankaflow/#ankaflow.internal.browser.RemoteObject.upload","title":"upload","text":"<pre><code>upload(remote_path: RemotePath, local_file: str)\n</code></pre> <p>Uploads a local file to a remote path (stub).</p> <p>Parameters:</p> Name Type Description Default <code>remote_path</code> <code>RemotePath</code> <p>Destination remote URI.</p> required <code>local_file</code> <code>str</code> <p>Path to the local file.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always.</p>"},{"location":"api/ankaflow/#ankaflow.internal.duckdb","title":"duckdb","text":""},{"location":"api/ankaflow/#ankaflow.internal.duckdb.Relation","title":"Relation","text":"<p>               Bases: <code>ABC</code></p> <p>Relation is an async shim similar to DuckDBPyRelation object</p>"},{"location":"api/ankaflow/#ankaflow.internal.duckdb.Relation.raw","title":"raw","text":"<pre><code>raw() -&gt; DuckDBPyRelation\n</code></pre> <p>Exposes underlying native DuckDBPyRelation with full api.</p> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>DuckDBPyRelation</p>"},{"location":"api/ankaflow/#ankaflow.internal.macros","title":"macros","text":""},{"location":"api/ankaflow/#ankaflow.internal.macros.Fn","title":"Fn","text":"<p>Miscellaneous utility and convenience macros (UDFs)</p>"},{"location":"api/ankaflow/#ankaflow.internal.macros.Fn.dt","title":"dt","text":"<pre><code>dt = \"\\n    (a, fail_on_error := FALSE) AS\\n    CASE\\n        -- Case 1: ISO string with optional timezone \u2192 strip and cast\\n        WHEN TRY_CAST(REGEXP_REPLACE(CAST(a AS TEXT), '(Z|[+-][0-9]{2}:[0-9]{2})$', '') AS TIMESTAMP) IS NOT NULL\\n            THEN CAST(REGEXP_REPLACE(CAST(a AS TEXT), '(Z|[+-][0-9]{2}:[0-9]{2})$', '') AS TIMESTAMP)\\n\\n        -- Case 2: Standard timestamp string\\n        WHEN TRY_CAST(a AS TIMESTAMP) IS NOT NULL\\n            THEN CAST(a AS TIMESTAMP)\\n\\n        -- Case 3: ISO-style date\\n        WHEN TRY_CAST(a AS DATE) IS NOT NULL\\n            THEN CAST(a AS TIMESTAMP)\\n\\n        -- Case 4: Unix time in seconds (int or float)\\n        WHEN TRY_CAST(a AS DOUBLE) IS NOT NULL\\n            AND CAST(a AS TEXT) ~ '^[0-9]+(\\\\.[0-9]+)?$'\\n            AND TRY_CAST(CAST(a AS DOUBLE) AS BIGINT) BETWEEN 1000000000 AND 9999999999\\n        THEN make_timestamp(CAST(CAST(a AS DOUBLE) * 1000000 AS BIGINT))  -- seconds \u2192 microseconds\\n\\n\\n        -- Case 5: nanoseconds (length &gt; 15)\\n        WHEN TRY_CAST(a AS BIGINT) IS NOT NULL\\n            AND CAST(a AS TEXT) ~ '^[0-9]+$'\\n            AND LENGTH(CAST(a AS TEXT)) &gt; 15\\n            THEN make_timestamp(CAST(TRY_CAST(a AS BIGINT) / 1000 AS BIGINT))\\n\\n        -- Case 6: milliseconds\\n        WHEN TRY_CAST(a AS BIGINT) IS NOT NULL\\n            AND CAST(a AS TEXT) ~ '^[0-9]+$'\\n            THEN make_timestamp(CAST(TRY_CAST(a AS BIGINT) * 1000 AS BIGINT))\\n\\n        -- Case 7: Explicit fail for other strings\\n        WHEN TYPEOF(a) = 'VARCHAR' AND LENGTH(CAST(a AS TEXT)) &gt; 1 AND fail_on_error = TRUE\\n            THEN CAST('Unsupported format - use Fn.dt(value, pattern)' AS TIMESTAMP)\\n\\n        -- Fallback\\n        ELSE make_timestamp(0)\\n    END,\\n    (value, pattern) AS (\\n            SELECT * FROM query(\\n                concat(\\n                    'SELECT STRPTIME(''',\\n\\n                    -- Strip TZ suffix from value\\n                    REGEXP_REPLACE(value, '(Z|[+-][0-9]{2}:[0-9]{2}|[A-Za-z/_]+)$', ''),\\n\\n                    ''',''',\\n\\n                    -- Auto-detect and convert human-readable patterns\\n                    CASE\\n                        WHEN POSITION('%' IN pattern) &gt; 0 THEN\\n                            REGEXP_REPLACE(REGEXP_REPLACE(pattern, '%z', ''), '%Z', '')\\n                        ELSE\\n                            REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(\\n                                pattern,\\n                                'YYYY', '%Y'),\\n                                'MM', '%m'),\\n                                'DD', '%d'),\\n                                'HH', '%H'),\\n                                'mm', '%M'),\\n                                'ss', '%S')\\n                    END,\\n\\n                    ''')'\\n                )\\n            )\\n        );\\n    \"\n</code></pre> <p>dt macro provides robust datetime parsing and normalization.</p> <p>Overload (a):     Accepts any input value and attempts to convert it to a TIMESTAMP.     - Supports TIMESTAMP_NS, TIMESTAMP, DATE, and BIGINT (UNIX ms).     - Automatically strips time zone suffixes (Z or \u00b1HH:MM) from strings.     - Fails with a descriptive error if input is unrecognized.</p> <p>Overload (value, pattern):     Dynamically parses a string using STRPTIME.     - Strips unsupported time zone suffixes (Z or \u00b1HH:MM) from input.     - Removes %z and %Z from format string, as DuckDB does not interpret timezones.     - Uses query() to compile a literal SQL expression for parsing.</p> <p>Returns:</p> Type Description <p>A DuckDB TIMESTAMP or a hard failure on invalid input.</p>"},{"location":"api/ankaflow/#ankaflow.internal.server","title":"server","text":""},{"location":"api/ankaflow/#ankaflow.internal.server.DDB","title":"DDB","text":"<pre><code>DDB(\n    connection_options: ConnectionConfiguration,\n    persisted: Optional[str] = None,\n)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models","title":"models","text":""},{"location":"api/ankaflow/#ankaflow.models.AuthType","title":"AuthType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BasicHandler","title":"BasicHandler","text":"<p>               Bases: <code>BaseModel</code></p> <p>A no-op response handler used when no special processing (like pagination or transformation) is required.</p> <p>Typically used for single-response REST endpoints where the entire payload is returned in one request.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal</code> <p>Specifies the handler type as BASIC.</p>"},{"location":"api/ankaflow/#ankaflow.models.BasicHandler.kind","title":"kind","text":"<pre><code>kind: Literal[BASIC]\n</code></pre> <p>Specifies the handler type as Basic.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQuery","title":"BigQuery","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.CSV","title":"CSV","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Clickhouse","title":"Clickhouse","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection","title":"Connection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.config","title":"config","text":"<pre><code>config: Optional[ConnectionConfiguration] = None\n</code></pre> <p>Optional configuration for the current connection. If not present then global configuration will be used.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.create_statement","title":"create_statement","text":"<pre><code>create_statement: Optional[str] = None\n</code></pre> <p>Create statement for given table. Must be in requested dialect.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.Connection.fields","title":"fields","text":"<pre><code>fields: Optional[Fields] = None\n</code></pre> <p>If set then schema is used to generate source structure in case actual source does not provide data in which case generation of ephemeral view fails.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Model type e.g. Deltatable, Bigquery, Clickhouse, Parquet, File Custom connections can be loaded from module.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.locator","title":"locator","text":"<pre><code>locator: str\n</code></pre> <p>Table name or file name or URI, or other identifier required by the connection.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.optimize","title":"optimize","text":"<pre><code>optimize: Optional[Union[str, int]] = 1\n</code></pre> <p>Use with Deltatable and other engines whether to optimize after each sink operation. With larger tables this may be a lengthy synchronous operation.</p> <p>Default value is optimize and vacuum with 7 day retention.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.optimize--deltatable","title":"Deltatable","text":"<p>Values are <code>optimize,vacuum,all,Literal[int]</code>.   If value is literal int is provided then parts older than   number of days will be removed. Note that this will override   default retention days.</p> <p>String options <code>vacuum,all</code> are equivalent to 0.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Any parameters that can be passed to connection.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.Connection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If true then schema is automatically detected from the input data and logged '</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>ClickHouse database configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ContentType","title":"ContentType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection","title":"CustomConnection","text":"<p>               Bases: <code>BaseModel</code></p> <p>Custom connection provider. Custom connection may implement its own logic but must derive from base Connection class, and expose tap(), sink(), sql() and show_schema() even if they are no-op.</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.classname","title":"classname","text":"<pre><code>classname: str\n</code></pre> <p>Name of the class to load from the module</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.module","title":"module","text":"<pre><code>module: str\n</code></pre> <p>Python module where the connection class is defined</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.params","title":"params","text":"<pre><code>params: dict = Field(default_factory=dict)\n</code></pre> <p>Free-form configuration parameters passed to the loaded class</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p>"},{"location":"api/ankaflow/#ankaflow.models.DataType","title":"DataType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.DatabaseConfig","title":"DatabaseConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for SQL database connection configurations.</p>"},{"location":"api/ankaflow/#ankaflow.models.DatabaseConfig.cluster","title":"cluster","text":"<pre><code>cluster: Optional[str] = None\n</code></pre> <p>Optional cluster name or identifier (used in ClickHouse and other distributed systems).</p>"},{"location":"api/ankaflow/#ankaflow.models.DatabaseConfig.database","title":"database","text":"<pre><code>database: Optional[str] = None\n</code></pre> <p>Database name.</p>"},{"location":"api/ankaflow/#ankaflow.models.DatabaseConfig.host","title":"host","text":"<pre><code>host: Optional[str] = None\n</code></pre> <p>Hostname or IP address of the database server.</p>"},{"location":"api/ankaflow/#ankaflow.models.DatabaseConfig.password","title":"password","text":"<pre><code>password: Optional[str] = None\n</code></pre> <p>Password for authentication.</p>"},{"location":"api/ankaflow/#ankaflow.models.DatabaseConfig.port","title":"port","text":"<pre><code>port: Optional[int | str] = None\n</code></pre> <p>Database port.</p>"},{"location":"api/ankaflow/#ankaflow.models.DatabaseConfig.username","title":"username","text":"<pre><code>username: Optional[str] = None\n</code></pre> <p>Username for authentication.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock","title":"Datablock","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.connection","title":"connection","text":"<pre><code>connection: Optional[\n    Union[\n        Rest,\n        Variable,\n        BigQuery,\n        Deltatable,\n        Parquet,\n        Clickhouse,\n        CustomConnection,\n        JSON,\n        CSV,\n        File,\n        Dimension,\n    ]\n] = Field(None, discriminator=\"kind\")\n</code></pre> <p>Defines how the data is read from / written to the target.</p> <p>Connection fields may contain templates and they will be recursively.</p> <p>Special construct is JSON&gt; which allows dynamically generating parameters as runtime:</p> <pre><code>- kind: source\n  name: source_name\n  connection:\n    kind: File\n    params: &gt;\n      JSON&gt;{\n        \"key\": \"value\",\n        \"dynkey\": &lt;&lt;API.property&gt;&gt;,\n      }\n</code></pre> <p>In the above app <code>params</code> are constructed as JSON string. It is possible to even construct parameter keys dynamically:</p> <pre><code>params: &gt;\n  JSON&gt;\n  {\n    &lt;% for number in [1,2,3] %&gt;\n    \"key_&lt;&lt; number &gt;&gt;\":&lt;&lt; number &gt;&gt;&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n  }\n</code></pre> <p>Above example results the following:</p> <pre><code>params: &gt;\n  {\n    \"key_1\": 1,\n    \"key_2\": 2,\n    \"key_3\": 3\n  }\n</code></pre> <p>JSON&gt; structure cannot contain nested JSON&gt; structures, the entire string following the JSON&gt; header must result a valid JSON.</p> <p>Inbuilt connections include:</p> <ul> <li>Deltatable (read)</li> </ul> <p>If connection is <code>Deltatable</code> then query is required to narrow   down data stored in the delta table. <code>FROM</code> clause must be <code>Deltatable</code>: <code>- kind: source     name: delta_tap     connection:       kind: Deltatable       locator: delta_table     query: &gt;       select * from Deltatable</code></p> <ul> <li>Deltatable (write)</li> </ul> <p>See also StorageOptions</p> <p>The following example writes data from preceding stage   to delta table, appending the data, partitions using   <code>part</code> column, and optimizes and vacuums immediately without   retention after write.   <code>- kind: sink     name: delta_sink     connection:       kind: Deltatable       locator: delta_table       optimize: 0       data_mode: append       partition:       - part_field</code></p> <ul> <li>Parquet (read/write)</li> <li>JSON (read/write; NOTE: write operation generates newline-delimited JSON)</li> <li>CSV (read/write)</li> <li> <p>Variable (read/write)</p> </li> <li> <p>File (read)</p> </li> </ul> <p>File can be read from a connected filesystem (including s3). File name   and file type must be specified in the pipeline context:</p> <pre><code>- `context.FileName`: file name relative to `file_prefix`\n- `context.FileType`: file type, one of CSV, XLSX, JSON, HTML\n</code></pre> <p>Any file reader configuration must be passed in <code>params</code>.</p> <ul> <li>Rest (bidirectional)</li> </ul> <p>Rest connections consist of two parts: Client and Request. Client contains   base URL and authentication (basic, digest, header and oauth2 are supported).</p> <p><code>- name: TheRest     kind: source     connection:       kind: Rest       client:         base_url: https://api.example.com         auth:  # Every request to given endpoint share the same authentication           method: basic           values:             username: TheName             password: ThePassword       request:         endpoint: /v1/some-path         content_type: application/json         method: post         query:  # Query parameters           date: &lt;&lt;API.dt(None).isoformat()&gt;&gt;         body:  # JSON payload           param: 1         response:           content_type: application/json           locator: \"JMESPath.to.data\"</code></p> <p>Any custom source (API) can be used as long as available via module.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.context","title":"context","text":"<pre><code>context: Optional[FlowContext] = None\n</code></pre> <p>@private Global context passed to given stage</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.explain","title":"explain","text":"<pre><code>explain: Optional[bool] = None\n</code></pre> <p>If set to true then SQL query explanation will be logged.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.fields","title":"fields","text":"<pre><code>fields: Optional[List[Field]] = None\n</code></pre> <p>Explicitly defined output fields.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Defines which action will be performed:</p> <ul> <li>source</li> <li>transform</li> <li>sink</li> <li>pipeline</li> <li>sql</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.kind--sink","title":"Sink","text":"<p>Sink reads output from previous stage and stores in specified location.</p> <p>NOTE: If query is supplied with the stage then sink uses output of the query rather than preceding stage directly. Subsequent sink will use preceding stage. If supplied query must create either view or table with same name as current stage.</p> <pre><code>- name: my_sink\nkind: sink\nconnection:\n    kind: Variable\nquery: &gt;\n    CREATE VIEW my_sink AS\n    SELECT\n    1 as foo\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.kind--pipeline","title":"Pipeline","text":"<p>If pipeline is preceded by any stage then the subpipeline will be executed as many times as there are rows in the previous stage output. This is useful if you want to run same pipeline with different parameters. Make sure the pipeline is preceded by source or transform producing required number of rows. If you need to run subpipeline only once there are two options:</p> <ol> <li>Place it to the top</li> <li>Preced with tranform producing single row only</li> </ol> <p>Each row is then passed to subpipeline in a special variable.</p> <p>Example pipeline iterating subpipeline 5 times:</p> <ul> <li>kind: transform       name: generate_5       # Generate 5 rows       query: &gt;           select unnest as current_variable from unnest(generate_series(1,5))       show: 5</li> <li>kind: pipeline     name: looped_5x     stages:         - kind: transform           name: inside_loop           # In query we can reference the value passed from parent pipeline           query: &gt;               select 'Currently running iteration: {API.look('loop_control.current_variable', variables)}' as value           show: 5</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>Currently unused: Name for the connection configuration: name, or URI.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.log_level","title":"log_level","text":"<pre><code>log_level: Optional[LogLevel] = None\n</code></pre> <p>Set logging level. All stages after (including current) will log with specified level. Possible values: INFO (default), DEBUG, WARNING. Log level will be reset to INFO after each pipeline (including nested pipelines).</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Name of the stage, must be unique across all stages in the pipeline and conform to the rules: Must start with letter, may contain lowercase letters, number and underscores. Name is used to reference this stage by other subsequent stages.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.on_error","title":"on_error","text":"<pre><code>on_error: str = 'fail'\n</code></pre> <p>If set to 'continue' then pipeline will not fail. Subsequent stages referring to failed one must handle missing data.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.query","title":"query","text":"<pre><code>query: Optional[str] = None\n</code></pre> <p>SQL Query or dictionary with custom source parameters. May contain {dynamic variables}.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.show","title":"show","text":"<pre><code>show: int = 0\n</code></pre> <p>If set to positive integer then given number of rows from this stage will get logged. If set to -1 then all rows will be loggged. Set to 0 to disable logging.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True then schema is logged</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.skip_if","title":"skip_if","text":"<pre><code>skip_if: Optional[Any] = None\n</code></pre> <p>Any value that can evaluated using bool(). or template string e.g. <code>&lt;&lt; True &gt;&gt;</code>. When the expression evaluates to True then the stage is skipped.</p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.stages","title":"stages","text":"<pre><code>stages: Optional[Stages] = None\n</code></pre> <p>Used when kind is <code>Flow</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Datablock.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then flow execution will be paused after the stage for given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow/#ankaflow.models.Deltatable","title":"Deltatable","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Dimension","title":"Dimension","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Dimension.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p>"},{"location":"api/ankaflow/#ankaflow.models.Field","title":"Field","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data fields (equivalent to database field)</p>"},{"location":"api/ankaflow/#ankaflow.models.Field.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Field name must follow rules set to SQL engine field names</p>"},{"location":"api/ankaflow/#ankaflow.models.Field.type","title":"type","text":"<pre><code>type: str\n</code></pre> <p>Any data type support by SQL engine</p>"},{"location":"api/ankaflow/#ankaflow.models.Fields","title":"Fields","text":"<p>               Bases: <code>RootModel[List[Field]]</code></p> <p>Iterable list-like collection of Fields.</p>"},{"location":"api/ankaflow/#ankaflow.models.File","title":"File","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.models.JSON","title":"JSON","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.LogLevel","title":"LogLevel","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.ModelType","title":"ModelType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator","title":"Paginator","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for paginated REST APIs.</p> <p>This handler generates repeated requests by incrementing a page-related parameter until no more data is available. The stopping condition is usually inferred from the number of records in the response being less than <code>page_size</code>, or from a total record count field.</p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator.increment","title":"increment","text":"<pre><code>increment: int\n</code></pre> <p>Page parameter increment. Original request configuration should include initial value e.g. <code>page_no=1</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator.kind","title":"kind","text":"<pre><code>kind: Literal[PAGINATOR]\n</code></pre> <p>Specifies the handler type as Paginator.</p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator.page_param","title":"page_param","text":"<pre><code>page_param: str\n</code></pre> <p>Page parameter in the request (query or body) This will be incremented from request to request</p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator.page_size","title":"page_size","text":"<pre><code>page_size: int\n</code></pre> <p>Page size should be explicitly defined. If response contains less records it is considered to be last page</p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator.param_locator","title":"param_locator","text":"<pre><code>param_locator: ParameterDisposition\n</code></pre> <p>Define where the parameter is located: body or query</p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then each page request is throttled given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow/#ankaflow.models.Paginator.total_records","title":"total_records","text":"<pre><code>total_records: Optional[str] = None\n</code></pre> <p>JMESPath to total records count in the response.</p>"},{"location":"api/ankaflow/#ankaflow.models.ParameterDisposition","title":"ParameterDisposition","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Parquet","title":"Parquet","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Request","title":"Request","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Request.body","title":"body","text":"<pre><code>body: Optional[Union[str, Dict]] = None\n</code></pre> <p>Request body parameters.</p> <p>This field accepts either: - A Python <code>dict</code> representing a direct key-value mapping, or - A Jinja-templated JSON string with magic <code>@json</code> prefix, e.g.: <code>@json{\"parameter\": \"value\"}</code></p> <p>The template will be rendered using the following custom delimiters: - <code>&lt;&lt; ... &gt;&gt;</code> for variable interpolation - <code>&lt;% ... %&gt;</code> for logic/control flow (e.g., for-loops) - <code>&lt;# ... #&gt;</code> for inline comments</p> <p>The template will be rendered before being parsed into a valid JSON object. This allows the use of dynamic expressions, filters, and control flow such as loops.</p>"},{"location":"api/ankaflow/#ankaflow.models.Request.body--example-with-looping","title":"Example with looping","text":"<p>Given:</p> <pre><code>variables = {\n    \"MyList\": [\n        {\"id\": 1, \"value\": 10},\n        {\"id\": 2, \"value\": 20}\n    ]\n}\n</code></pre> <p>You can generate a dynamic body with:</p> <pre><code>body: &gt;\n@json[\n    &lt;% for row in API.look(\"MyTable\", variables) %&gt;\n        { \"id\": &lt;&lt; row.id &gt;&gt;, \"value\": &lt;&lt; row.value &gt;&gt; }&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n]\n</code></pre> <p>This will render to a proper JSON list:</p> <pre><code>[\n    { \"id\": 1, \"value\": 10 },\n    { \"id\": 2, \"value\": 20 }\n]\n</code></pre> <p>Notes: - When using @json, the entire string is rendered as a Jinja template     and then parsed with json.loads(). - Nested @json blocks are not supported. - Newlines and whitespace are automatically collapsed during rendering.</p>"},{"location":"api/ankaflow/#ankaflow.models.Request.content_type","title":"content_type","text":"<pre><code>content_type: ContentType = JSON\n</code></pre> <p>Request content type</p>"},{"location":"api/ankaflow/#ankaflow.models.Request.endpoint","title":"endpoint","text":"<pre><code>endpoint: str\n</code></pre> <p>Request endpoint e.g. <code>get/data</code> under base url:</p> <p>Example <code>https://api.example.com/v1</code> + <code>get/data</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Request.errorhandler","title":"errorhandler","text":"<pre><code>errorhandler: RestErrorHandler = Field(\n    default_factory=RestErrorHandler\n)\n</code></pre> <p>Custom error handler e.g. for searching conditions in response or custom status codes</p>"},{"location":"api/ankaflow/#ankaflow.models.Request.method","title":"method","text":"<pre><code>method: RequestMethod\n</code></pre> <p>Request method e.g. <code>post,get,put</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Request.query","title":"query","text":"<pre><code>query: Dict = {}\n</code></pre> <p>Query parameters. Parameters may contain template variables.</p>"},{"location":"api/ankaflow/#ankaflow.models.Request.response","title":"response","text":"<pre><code>response: RestResponse\n</code></pre> <p>Response handling configuration</p>"},{"location":"api/ankaflow/#ankaflow.models.RequestMethod","title":"RequestMethod","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.ResponseHandlerTypes","title":"ResponseHandlerTypes","text":""},{"location":"api/ankaflow/#ankaflow.models.Rest","title":"Rest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a REST-based data connection.</p> <p>This model defines how to configure and execute REST API requests as part of a pipeline step. It includes the request definition, client behavior (e.g., retries, headers), and optional schema discovery.</p>"},{"location":"api/ankaflow/#ankaflow.models.Rest.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre> <p>Configuration for the REST client (e.g., base URL, headers, auth).</p>"},{"location":"api/ankaflow/#ankaflow.models.Rest.fields","title":"fields","text":"<pre><code>fields: Optional[List[Field]] = None\n</code></pre> <p>Optional schema definition used to validate or transform response data.</p> <p>It is recommended to manually specify the schema after initial discovery. This ensures downstream pipeline stages remain stable, even when the remote API returns no results (e.g., due to no updates in an incremental fetch). Explicit schema prevents silent failures or schema drift in such cases.</p>"},{"location":"api/ankaflow/#ankaflow.models.Rest.kind","title":"kind","text":"<pre><code>kind: Literal['Rest']\n</code></pre> <p>Specifies the connection type as \"Rest\".</p>"},{"location":"api/ankaflow/#ankaflow.models.Rest.request","title":"request","text":"<pre><code>request: Request\n</code></pre> <p>Request template specifying method, path, body, etc.</p>"},{"location":"api/ankaflow/#ankaflow.models.Rest.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True, the connector will attempt to infer or display the response schema automatically.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestAuth","title":"RestAuth","text":"<p>               Bases: <code>BaseModel</code></p> <p>Authenctication configuration for Rest connection.</p> <p>NOTE: Not all authentication methods may not work in browser due to limitations in the network API.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestAuth.method","title":"method","text":"<pre><code>method: AuthType\n</code></pre> <p>Specifies authentiation type.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestAuth.values","title":"values","text":"<pre><code>values: StringDict\n</code></pre> <p>Mapping of parameter names and values.</p> <p>{     'X-Auth-Token': '' }"},{"location":"api/ankaflow/#ankaflow.models.RestAuth.coerce_to_stringdict","title":"coerce_to_stringdict","text":"<pre><code>coerce_to_stringdict(v)\n</code></pre> <p>Rest header values must be strings. This convenience validator  automatically converts regiular dictionary to StringDict.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestClientConfig","title":"RestClientConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rest client for given base URL. Includes transport and authentication configuration</p>"},{"location":"api/ankaflow/#ankaflow.models.RestClientConfig.base_url","title":"base_url","text":"<pre><code>base_url: str\n</code></pre> <p>Base URL, typically server or API root. All endpoints with the same base URL share the same authentication.</p> <p>Example: <code>https://api.example.com/v1</code></p>"},{"location":"api/ankaflow/#ankaflow.models.RestClientConfig.timeout","title":"timeout","text":"<pre><code>timeout: Optional[float] = None\n</code></pre> <p>Request timeout in seconds. Default is 5. Set 0 to disable timout.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestErrorHandler","title":"RestErrorHandler","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.RestErrorHandler.condition","title":"condition","text":"<pre><code>condition: Optional[str] = None\n</code></pre> <p>JMESPath expression to look for in the response body. Error will be generated if expression evaluates to True</p>"},{"location":"api/ankaflow/#ankaflow.models.RestErrorHandler.error_status_codes","title":"error_status_codes","text":"<pre><code>error_status_codes: List[int] = []\n</code></pre> <p>List of HTTP status codes to be treated as errors.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestErrorHandler.message","title":"message","text":"<pre><code>message: Optional[str] = None\n</code></pre> <p>JMESPath expression to extract error message from respose. If omitted entire response will be included in error.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestResponse","title":"RestResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response configuration. Response can be paged, polled URL or in body.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestResponse.content_type","title":"content_type","text":"<pre><code>content_type: DataType\n</code></pre> <p>Returned data type</p>"},{"location":"api/ankaflow/#ankaflow.models.RestResponse.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>JMESPath to read data from JSON body. If not set then entire body is treated as data.</p>"},{"location":"api/ankaflow/#ankaflow.models.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow/#ankaflow.models.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.models.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Datablock]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Datablock]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Datablock]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p>"},{"location":"api/ankaflow/#ankaflow.models.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Datablock]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Datablock]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Datablock]</code> <p>from first to last.</p>"},{"location":"api/ankaflow/#ankaflow.models.StatePoller","title":"StatePoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for state-based polling APIs.</p> <p>This handler is designed for asynchronous workflows where the client repeatedly polls an endpoint until a certain state is reached (e.g., job completion, resource readiness). Once the condition is met, the pipeline continues by reading from the final data <code>locator</code>.</p>"},{"location":"api/ankaflow/#ankaflow.models.StatePoller.kind","title":"kind","text":"<pre><code>kind: Literal[STATEPOLLING]\n</code></pre> <p>Specifies the handler type as StatePolling.</p>"},{"location":"api/ankaflow/#ankaflow.models.StatePoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: str\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"},{"location":"api/ankaflow/#ankaflow.models.URLPoller","title":"URLPoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>URL Poller makes request(s) to remote API until an URL is returned</p>"},{"location":"api/ankaflow/#ankaflow.models.URLPoller.kind","title":"kind","text":"<pre><code>kind: Literal[URLPOLLING]\n</code></pre> <p>Specifies the handler type as URLPolling.</p>"},{"location":"api/ankaflow/#ankaflow.models.URLPoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: Optional[str] = None\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"},{"location":"api/ankaflow/#ankaflow.models.Variable","title":"Variable","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"},{"location":"api/ankaflow/#ankaflow.tests","title":"tests","text":""},{"location":"api/ankaflow.models/","title":"Pipeline API","text":"<p>Pipeline API provides building blocks for YAML pipeline.</p> <p>Each stage and configuration object here represents a specific part of the YAML definition file.</p> <p>Example pipeline stage with all possible keys. Actual usage of keys depends on used model <code>kind</code>.</p> <pre><code>- name: str\n  kind: source | sink | transform\n  log_level: DEBUG\n  skip_if: \n  on_error: error | continue\n  connection: Connection\n    kind: Parquet | Deltatable | Rest | ...\n    locator: str\n    config: ConnectionConfiguration\n    client: RestClient\n    request: RestRequest\n      endpoint: str\n      method: RequestMethod\n      errorhandler: RestErrorHandler\n      auth: RestAuth\n        method: header | oauth2 | ...\n        values: StringDict\n      query: dict | @json-magic\n      body: dict | @json-magic\n      response: RestResponse\n        handler: \n            kind: ResponseHandlerTypes\n            page_param: str\n            page_size: str\n            param_locator: ParameterDisposition\n            total_records: str\n            throttle: int | float \n        content_type: DataType\n        locator: str\n    fields: Fields\n  show: int\n  show_schema: bool\n  query: &gt; str\n  stages: Stages\n  throttle: int\n\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.AuthType","title":"AuthType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BasicHandler","title":"BasicHandler","text":"<p>               Bases: <code>BaseModel</code></p> <p>A no-op response handler used when no special processing (like pagination or transformation) is required.</p> <p>Typically used for single-response REST endpoints where the entire payload is returned in one request.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal</code> <p>Specifies the handler type as BASIC.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BasicHandler.kind","title":"kind","text":"<pre><code>kind: Literal[BASIC]\n</code></pre> <p>Specifies the handler type as Basic.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQuery","title":"BigQuery","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.CSV","title":"CSV","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Clickhouse","title":"Clickhouse","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection","title":"Connection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.config","title":"config","text":"<pre><code>config: Optional[ConnectionConfiguration] = None\n</code></pre> <p>Optional configuration for the current connection. If not present then global configuration will be used.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.create_statement","title":"create_statement","text":"<pre><code>create_statement: Optional[str] = None\n</code></pre> <p>Create statement for given table. Must be in requested dialect.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.fields","title":"fields","text":"<pre><code>fields: Optional[Fields] = None\n</code></pre> <p>If set then schema is used to generate source structure in case actual source does not provide data in which case generation of ephemeral view fails.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Model type e.g. Deltatable, Bigquery, Clickhouse, Parquet, File Custom connections can be loaded from module.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.locator","title":"locator","text":"<pre><code>locator: str\n</code></pre> <p>Table name or file name or URI, or other identifier required by the connection.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.optimize","title":"optimize","text":"<pre><code>optimize: Optional[Union[str, int]] = 1\n</code></pre> <p>Use with Deltatable and other engines whether to optimize after each sink operation. With larger tables this may be a lengthy synchronous operation.</p> <p>Default value is optimize and vacuum with 7 day retention.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.optimize--deltatable","title":"Deltatable","text":"<p>Values are <code>optimize,vacuum,all,Literal[int]</code>.   If value is literal int is provided then parts older than   number of days will be removed. Note that this will override   default retention days.</p> <p>String options <code>vacuum,all</code> are equivalent to 0.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Any parameters that can be passed to connection.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If true then schema is automatically detected from the input data and logged '</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>ClickHouse database configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ContentType","title":"ContentType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection","title":"CustomConnection","text":"<p>               Bases: <code>BaseModel</code></p> <p>Custom connection provider. Custom connection may implement its own logic but must derive from base Connection class, and expose tap(), sink(), sql() and show_schema() even if they are no-op.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.classname","title":"classname","text":"<pre><code>classname: str\n</code></pre> <p>Name of the class to load from the module</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.module","title":"module","text":"<pre><code>module: str\n</code></pre> <p>Python module where the connection class is defined</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.params","title":"params","text":"<pre><code>params: dict = Field(default_factory=dict)\n</code></pre> <p>Free-form configuration parameters passed to the loaded class</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DataType","title":"DataType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.DatabaseConfig","title":"DatabaseConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for SQL database connection configurations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DatabaseConfig.cluster","title":"cluster","text":"<pre><code>cluster: Optional[str] = None\n</code></pre> <p>Optional cluster name or identifier (used in ClickHouse and other distributed systems).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DatabaseConfig.database","title":"database","text":"<pre><code>database: Optional[str] = None\n</code></pre> <p>Database name.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DatabaseConfig.host","title":"host","text":"<pre><code>host: Optional[str] = None\n</code></pre> <p>Hostname or IP address of the database server.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DatabaseConfig.password","title":"password","text":"<pre><code>password: Optional[str] = None\n</code></pre> <p>Password for authentication.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DatabaseConfig.port","title":"port","text":"<pre><code>port: Optional[int | str] = None\n</code></pre> <p>Database port.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DatabaseConfig.username","title":"username","text":"<pre><code>username: Optional[str] = None\n</code></pre> <p>Username for authentication.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock","title":"Datablock","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.connection","title":"connection","text":"<pre><code>connection: Optional[\n    Union[\n        Rest,\n        Variable,\n        BigQuery,\n        Deltatable,\n        Parquet,\n        Clickhouse,\n        CustomConnection,\n        JSON,\n        CSV,\n        File,\n        Dimension,\n    ]\n] = Field(None, discriminator=\"kind\")\n</code></pre> <p>Defines how the data is read from / written to the target.</p> <p>Connection fields may contain templates and they will be recursively.</p> <p>Special construct is JSON&gt; which allows dynamically generating parameters as runtime:</p> <pre><code>- kind: source\n  name: source_name\n  connection:\n    kind: File\n    params: &gt;\n      JSON&gt;{\n        \"key\": \"value\",\n        \"dynkey\": &lt;&lt;API.property&gt;&gt;,\n      }\n</code></pre> <p>In the above app <code>params</code> are constructed as JSON string. It is possible to even construct parameter keys dynamically:</p> <pre><code>params: &gt;\n  JSON&gt;\n  {\n    &lt;% for number in [1,2,3] %&gt;\n    \"key_&lt;&lt; number &gt;&gt;\":&lt;&lt; number &gt;&gt;&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n  }\n</code></pre> <p>Above example results the following:</p> <pre><code>params: &gt;\n  {\n    \"key_1\": 1,\n    \"key_2\": 2,\n    \"key_3\": 3\n  }\n</code></pre> <p>JSON&gt; structure cannot contain nested JSON&gt; structures, the entire string following the JSON&gt; header must result a valid JSON.</p> <p>Inbuilt connections include:</p> <ul> <li>Deltatable (read)</li> </ul> <p>If connection is <code>Deltatable</code> then query is required to narrow   down data stored in the delta table. <code>FROM</code> clause must be <code>Deltatable</code>: <code>- kind: source     name: delta_tap     connection:       kind: Deltatable       locator: delta_table     query: &gt;       select * from Deltatable</code></p> <ul> <li>Deltatable (write)</li> </ul> <p>See also StorageOptions</p> <p>The following example writes data from preceding stage   to delta table, appending the data, partitions using   <code>part</code> column, and optimizes and vacuums immediately without   retention after write.   <code>- kind: sink     name: delta_sink     connection:       kind: Deltatable       locator: delta_table       optimize: 0       data_mode: append       partition:       - part_field</code></p> <ul> <li>Parquet (read/write)</li> <li>JSON (read/write; NOTE: write operation generates newline-delimited JSON)</li> <li>CSV (read/write)</li> <li> <p>Variable (read/write)</p> </li> <li> <p>File (read)</p> </li> </ul> <p>File can be read from a connected filesystem (including s3). File name   and file type must be specified in the pipeline context:</p> <pre><code>- `context.FileName`: file name relative to `file_prefix`\n- `context.FileType`: file type, one of CSV, XLSX, JSON, HTML\n</code></pre> <p>Any file reader configuration must be passed in <code>params</code>.</p> <ul> <li>Rest (bidirectional)</li> </ul> <p>Rest connections consist of two parts: Client and Request. Client contains   base URL and authentication (basic, digest, header and oauth2 are supported).</p> <p><code>- name: TheRest     kind: source     connection:       kind: Rest       client:         base_url: https://api.example.com         auth:  # Every request to given endpoint share the same authentication           method: basic           values:             username: TheName             password: ThePassword       request:         endpoint: /v1/some-path         content_type: application/json         method: post         query:  # Query parameters           date: &lt;&lt;API.dt(None).isoformat()&gt;&gt;         body:  # JSON payload           param: 1         response:           content_type: application/json           locator: \"JMESPath.to.data\"</code></p> <p>Any custom source (API) can be used as long as available via module.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.context","title":"context","text":"<pre><code>context: Optional[FlowContext] = None\n</code></pre> <p>@private Global context passed to given stage</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.explain","title":"explain","text":"<pre><code>explain: Optional[bool] = None\n</code></pre> <p>If set to true then SQL query explanation will be logged.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.fields","title":"fields","text":"<pre><code>fields: Optional[List[Field]] = None\n</code></pre> <p>Explicitly defined output fields.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Defines which action will be performed:</p> <ul> <li>source</li> <li>transform</li> <li>sink</li> <li>pipeline</li> <li>sql</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.kind--sink","title":"Sink","text":"<p>Sink reads output from previous stage and stores in specified location.</p> <p>NOTE: If query is supplied with the stage then sink uses output of the query rather than preceding stage directly. Subsequent sink will use preceding stage. If supplied query must create either view or table with same name as current stage.</p> <pre><code>- name: my_sink\nkind: sink\nconnection:\n    kind: Variable\nquery: &gt;\n    CREATE VIEW my_sink AS\n    SELECT\n    1 as foo\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.kind--pipeline","title":"Pipeline","text":"<p>If pipeline is preceded by any stage then the subpipeline will be executed as many times as there are rows in the previous stage output. This is useful if you want to run same pipeline with different parameters. Make sure the pipeline is preceded by source or transform producing required number of rows. If you need to run subpipeline only once there are two options:</p> <ol> <li>Place it to the top</li> <li>Preced with tranform producing single row only</li> </ol> <p>Each row is then passed to subpipeline in a special variable.</p> <p>Example pipeline iterating subpipeline 5 times:</p> <ul> <li>kind: transform       name: generate_5       # Generate 5 rows       query: &gt;           select unnest as current_variable from unnest(generate_series(1,5))       show: 5</li> <li>kind: pipeline     name: looped_5x     stages:         - kind: transform           name: inside_loop           # In query we can reference the value passed from parent pipeline           query: &gt;               select 'Currently running iteration: {API.look('loop_control.current_variable', variables)}' as value           show: 5</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>Currently unused: Name for the connection configuration: name, or URI.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.log_level","title":"log_level","text":"<pre><code>log_level: Optional[LogLevel] = None\n</code></pre> <p>Set logging level. All stages after (including current) will log with specified level. Possible values: INFO (default), DEBUG, WARNING. Log level will be reset to INFO after each pipeline (including nested pipelines).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Name of the stage, must be unique across all stages in the pipeline and conform to the rules: Must start with letter, may contain lowercase letters, number and underscores. Name is used to reference this stage by other subsequent stages.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.on_error","title":"on_error","text":"<pre><code>on_error: str = 'fail'\n</code></pre> <p>If set to 'continue' then pipeline will not fail. Subsequent stages referring to failed one must handle missing data.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.query","title":"query","text":"<pre><code>query: Optional[str] = None\n</code></pre> <p>SQL Query or dictionary with custom source parameters. May contain {dynamic variables}.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.show","title":"show","text":"<pre><code>show: int = 0\n</code></pre> <p>If set to positive integer then given number of rows from this stage will get logged. If set to -1 then all rows will be loggged. Set to 0 to disable logging.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True then schema is logged</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.skip_if","title":"skip_if","text":"<pre><code>skip_if: Optional[Any] = None\n</code></pre> <p>Any value that can evaluated using bool(). or template string e.g. <code>&lt;&lt; True &gt;&gt;</code>. When the expression evaluates to True then the stage is skipped.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.stages","title":"stages","text":"<pre><code>stages: Optional[Stages] = None\n</code></pre> <p>Used when kind is <code>Flow</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Datablock.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then flow execution will be paused after the stage for given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Deltatable","title":"Deltatable","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Dimension","title":"Dimension","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Dimension.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Field","title":"Field","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data fields (equivalent to database field)</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Field.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Field name must follow rules set to SQL engine field names</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Field.type","title":"type","text":"<pre><code>type: str\n</code></pre> <p>Any data type support by SQL engine</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Fields","title":"Fields","text":"<p>               Bases: <code>RootModel[List[Field]]</code></p> <p>Iterable list-like collection of Fields.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.File","title":"File","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.JSON","title":"JSON","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Loadable","title":"Loadable","text":"<p>               Bases: <code>Protocol</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.LogLevel","title":"LogLevel","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.ModelType","title":"ModelType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator","title":"Paginator","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for paginated REST APIs.</p> <p>This handler generates repeated requests by incrementing a page-related parameter until no more data is available. The stopping condition is usually inferred from the number of records in the response being less than <code>page_size</code>, or from a total record count field.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator.increment","title":"increment","text":"<pre><code>increment: int\n</code></pre> <p>Page parameter increment. Original request configuration should include initial value e.g. <code>page_no=1</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator.kind","title":"kind","text":"<pre><code>kind: Literal[PAGINATOR]\n</code></pre> <p>Specifies the handler type as Paginator.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator.page_param","title":"page_param","text":"<pre><code>page_param: str\n</code></pre> <p>Page parameter in the request (query or body) This will be incremented from request to request</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator.page_size","title":"page_size","text":"<pre><code>page_size: int\n</code></pre> <p>Page size should be explicitly defined. If response contains less records it is considered to be last page</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator.param_locator","title":"param_locator","text":"<pre><code>param_locator: ParameterDisposition\n</code></pre> <p>Define where the parameter is located: body or query</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then each page request is throttled given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Paginator.total_records","title":"total_records","text":"<pre><code>total_records: Optional[str] = None\n</code></pre> <p>JMESPath to total records count in the response.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ParameterDisposition","title":"ParameterDisposition","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Parquet","title":"Parquet","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request","title":"Request","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.body","title":"body","text":"<pre><code>body: Optional[Union[str, Dict]] = None\n</code></pre> <p>Request body parameters.</p> <p>This field accepts either: - A Python <code>dict</code> representing a direct key-value mapping, or - A Jinja-templated JSON string with magic <code>@json</code> prefix, e.g.: <code>@json{\"parameter\": \"value\"}</code></p> <p>The template will be rendered using the following custom delimiters: - <code>&lt;&lt; ... &gt;&gt;</code> for variable interpolation - <code>&lt;% ... %&gt;</code> for logic/control flow (e.g., for-loops) - <code>&lt;# ... #&gt;</code> for inline comments</p> <p>The template will be rendered before being parsed into a valid JSON object. This allows the use of dynamic expressions, filters, and control flow such as loops.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.body--example-with-looping","title":"Example with looping","text":"<p>Given:</p> <pre><code>variables = {\n    \"MyList\": [\n        {\"id\": 1, \"value\": 10},\n        {\"id\": 2, \"value\": 20}\n    ]\n}\n</code></pre> <p>You can generate a dynamic body with:</p> <pre><code>body: &gt;\n@json[\n    &lt;% for row in API.look(\"MyTable\", variables) %&gt;\n        { \"id\": &lt;&lt; row.id &gt;&gt;, \"value\": &lt;&lt; row.value &gt;&gt; }&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n]\n</code></pre> <p>This will render to a proper JSON list:</p> <pre><code>[\n    { \"id\": 1, \"value\": 10 },\n    { \"id\": 2, \"value\": 20 }\n]\n</code></pre> <p>Notes: - When using @json, the entire string is rendered as a Jinja template     and then parsed with json.loads(). - Nested @json blocks are not supported. - Newlines and whitespace are automatically collapsed during rendering.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.content_type","title":"content_type","text":"<pre><code>content_type: ContentType = JSON\n</code></pre> <p>Request content type</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.endpoint","title":"endpoint","text":"<pre><code>endpoint: str\n</code></pre> <p>Request endpoint e.g. <code>get/data</code> under base url:</p> <p>Example <code>https://api.example.com/v1</code> + <code>get/data</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.errorhandler","title":"errorhandler","text":"<pre><code>errorhandler: RestErrorHandler = Field(\n    default_factory=RestErrorHandler\n)\n</code></pre> <p>Custom error handler e.g. for searching conditions in response or custom status codes</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.method","title":"method","text":"<pre><code>method: RequestMethod\n</code></pre> <p>Request method e.g. <code>post,get,put</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.query","title":"query","text":"<pre><code>query: Dict = {}\n</code></pre> <p>Query parameters. Parameters may contain template variables.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Request.response","title":"response","text":"<pre><code>response: RestResponse\n</code></pre> <p>Response handling configuration</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RequestMethod","title":"RequestMethod","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.ResponseHandlerTypes","title":"ResponseHandlerTypes","text":""},{"location":"api/ankaflow.models/#ankaflow.models.Rest","title":"Rest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a REST-based data connection.</p> <p>This model defines how to configure and execute REST API requests as part of a pipeline step. It includes the request definition, client behavior (e.g., retries, headers), and optional schema discovery.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Rest.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre> <p>Configuration for the REST client (e.g., base URL, headers, auth).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Rest.fields","title":"fields","text":"<pre><code>fields: Optional[List[Field]] = None\n</code></pre> <p>Optional schema definition used to validate or transform response data.</p> <p>It is recommended to manually specify the schema after initial discovery. This ensures downstream pipeline stages remain stable, even when the remote API returns no results (e.g., due to no updates in an incremental fetch). Explicit schema prevents silent failures or schema drift in such cases.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Rest.kind","title":"kind","text":"<pre><code>kind: Literal['Rest']\n</code></pre> <p>Specifies the connection type as \"Rest\".</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Rest.request","title":"request","text":"<pre><code>request: Request\n</code></pre> <p>Request template specifying method, path, body, etc.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Rest.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True, the connector will attempt to infer or display the response schema automatically.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestAuth","title":"RestAuth","text":"<p>               Bases: <code>BaseModel</code></p> <p>Authenctication configuration for Rest connection.</p> <p>NOTE: Not all authentication methods may not work in browser due to limitations in the network API.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestAuth.method","title":"method","text":"<pre><code>method: AuthType\n</code></pre> <p>Specifies authentiation type.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestAuth.values","title":"values","text":"<pre><code>values: StringDict\n</code></pre> <p>Mapping of parameter names and values.</p> <p>{     'X-Auth-Token': '' }"},{"location":"api/ankaflow.models/#ankaflow.models.RestAuth.coerce_to_stringdict","title":"coerce_to_stringdict","text":"<pre><code>coerce_to_stringdict(v)\n</code></pre> <p>Rest header values must be strings. This convenience validator  automatically converts regiular dictionary to StringDict.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestClientConfig","title":"RestClientConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rest client for given base URL. Includes transport and authentication configuration</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestClientConfig.base_url","title":"base_url","text":"<pre><code>base_url: str\n</code></pre> <p>Base URL, typically server or API root. All endpoints with the same base URL share the same authentication.</p> <p>Example: <code>https://api.example.com/v1</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestClientConfig.timeout","title":"timeout","text":"<pre><code>timeout: Optional[float] = None\n</code></pre> <p>Request timeout in seconds. Default is 5. Set 0 to disable timout.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestErrorHandler","title":"RestErrorHandler","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestErrorHandler.condition","title":"condition","text":"<pre><code>condition: Optional[str] = None\n</code></pre> <p>JMESPath expression to look for in the response body. Error will be generated if expression evaluates to True</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestErrorHandler.error_status_codes","title":"error_status_codes","text":"<pre><code>error_status_codes: List[int] = []\n</code></pre> <p>List of HTTP status codes to be treated as errors.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestErrorHandler.message","title":"message","text":"<pre><code>message: Optional[str] = None\n</code></pre> <p>JMESPath expression to extract error message from respose. If omitted entire response will be included in error.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestResponse","title":"RestResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response configuration. Response can be paged, polled URL or in body.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestResponse.content_type","title":"content_type","text":"<pre><code>content_type: DataType\n</code></pre> <p>Returned data type</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestResponse.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>JMESPath to read data from JSON body. If not set then entire body is treated as data.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.SchemaItem","title":"SchemaItem","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Datablock]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Datablock]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Datablock]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Datablock]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Datablock]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Datablock]</code> <p>from first to last.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.StatePoller","title":"StatePoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for state-based polling APIs.</p> <p>This handler is designed for asynchronous workflows where the client repeatedly polls an endpoint until a certain state is reached (e.g., job completion, resource readiness). Once the condition is met, the pipeline continues by reading from the final data <code>locator</code>.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.StatePoller.kind","title":"kind","text":"<pre><code>kind: Literal[STATEPOLLING]\n</code></pre> <p>Specifies the handler type as StatePolling.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.StatePoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: str\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.URLPoller","title":"URLPoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>URL Poller makes request(s) to remote API until an URL is returned</p>"},{"location":"api/ankaflow.models/#ankaflow.models.URLPoller.kind","title":"kind","text":"<pre><code>kind: Literal[URLPOLLING]\n</code></pre> <p>Specifies the handler type as URLPolling.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.URLPoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: Optional[str] = None\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Variable","title":"Variable","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"}]}